{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAISHNAVI JAMDADE(TM39453)\n",
    "### HOMEWORK 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the file into numpy array of dimensions (60000,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertfunc(file):\n",
    "    arr = idx2numpy.convert_from_file(file)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=convertfunc('train-images.idx3-ubyte')\n",
    "y_train=convertfunc('train-labels.idx1-ubyte')\n",
    "X_test=convertfunc('t10k-images.idx3-ubyte')\n",
    "y_test=convertfunc('t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapefunc(array,rows,columns):\n",
    "    array=array.reshape(rows,columns)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=reshapefunc(X_train,60000,784)\n",
    "X_test=reshapefunc(X_test,10000,784)\n",
    "#y_train=reshapefunc(y_train,1,60000)\n",
    "#y_test=reshapefunc(y_test,1,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape, y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Training data into Training and Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Question 3, Here we have split the original X_train(training dataset) into X_trainval and X_testval where we will train our model with 4 configurations on our X_trainval data, and then evaluate it on our Validation dataset that is, X_testval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval,X_testval,y_trainval,y_testval=train_test_split(X_train,y_train,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (18000, 784), (42000,), (18000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainval.shape, X_testval.shape, y_trainval.shape,y_testval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One hot label encoding for output labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using one hot label encoding for validation output labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainval = pd.get_dummies(y_trainval)\n",
    "y_trainval=np.array(y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(inputs, hNeurons, oNeurons):\n",
    "\n",
    "    weights1=np.random.randn(inputs,hNeurons)*np.sqrt(1./inputs)    #784 rows 5 columns\n",
    "    bias1=np.zeros((1, hNeurons))*np.sqrt(1./inputs)\n",
    "    #hence x.w = 1,5 dimensions + bias [1,5]\n",
    "    weights2=np.random.randn(hNeurons,oNeurons)*np.sqrt(1./hNeurons)\n",
    "    #hence x.w= 5,10 dimensions + bias [1,10]\n",
    "    bias2= np.zeros((1, oNeurons))*np.sqrt(1./hNeurons)\n",
    "\n",
    "    #ouptut vecotr is going to be of dimensions [1,10] then softmax\n",
    "    #W1.shape , b1.shape, W2.shape, b2.shape\n",
    "    \n",
    "    return weights1, bias1, weights2, bias2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    act = 1. / (1. + np.exp(-z))\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Activation Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    tanhact=(np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    return tanhact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising the values:\n",
    "\n",
    "def softmax(z):\n",
    "    softact = np.exp(z)/np.sum(np.exp(z), axis=1,keepdims=True)                                       \n",
    "    return softact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(X_t,W1,b1,W2,b2,actfunc):\n",
    "    z1= np.dot(X_t,W1)+b1\n",
    "    h1=actfunc(z1)\n",
    "    \n",
    "    z2=np.dot(h1,W2)+b2\n",
    "    output=softmax(z2)\n",
    "    return z1, h1, z2, output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(ypred,y_t):     \n",
    "    n_samples = y_t.shape[0]\n",
    "    L = (ypred-y_t)/n_samples\n",
    "    return L    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(ypred,y_t):\n",
    "    L_sum = np.sum(np.multiply(y_t, np.log(ypred)))\n",
    "    num_samples = y_t.shape[0]\n",
    "    L = -(1./num_samples) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate Sigmoid Derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(h1):\n",
    "    deriv_s=h1*(1-h1)\n",
    "    return deriv_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate Tanh derivative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(h1):\n",
    "    deriv_t=1-h1**2\n",
    "    return deriv_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Function to compute Gradient Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(h1,W2,ypred,y_t,X_t,derivfunc):\n",
    "    \n",
    "    #Calculate error\n",
    "    L=error(ypred,y_t)\n",
    "    \n",
    "    # z1, h1, z2, output\n",
    "    # z1, and z2 => dot products\n",
    "    # h1 and output => activations\n",
    "    change_output=L\n",
    "    #dL/dW2\n",
    "    delta_W2= np.dot(h1.T,change_output)\n",
    "    \n",
    "    #dL/db2\n",
    "    delta_b2= np.sum(change_output,axis=0,keepdims=True)\n",
    "    \n",
    "    #dL/dh1\n",
    "    delta_h1=np.dot(change_output,W2.T)\n",
    "\n",
    "    change_h1= delta_h1*derivfunc(h1)\n",
    "    \n",
    "    #dL/dW1\n",
    "    delta_W1= np.dot(X_t.T, change_h1)\n",
    "    \n",
    "    #dL/db1\n",
    "    delta_b1= np.sum(change_h1,axis=0,keepdims=True)\n",
    "    \n",
    "    return delta_W2, delta_b2, delta_W1, delta_b1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(alpha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1):\n",
    "    W2= W2- alpha*(delta_W2)\n",
    "    b2= b2 - alpha*(delta_b2)\n",
    "    W1= W1- alpha*(delta_W1)\n",
    "    b1=b1- alpha*(delta_b1)\n",
    "    \n",
    "    return W2,b2,W1,b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X_t,y_t,neurons,lr,iterations,actfunc,derivfunc):\n",
    "    hNeurons=neurons                              #hidden neuron\n",
    "    inputs=X_t.shape[1]                   #number of inputs\n",
    "    oNeurons=y_t.shape[1]\n",
    "    alpha= lr                       #learning rate\n",
    "    epochs=iterations\n",
    "    \n",
    "    #retriving the actual labels for y_train:\n",
    "    labely_train=np.argmax(y_t,axis=1)\n",
    "    \n",
    "    #initialize weights\n",
    "    W1, b1, W2, b2= initialize_weights(inputs,hNeurons,oNeurons)\n",
    "    li=[]\n",
    "    listloss=[]\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #feed forward\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_t,W1,b1,W2,b2,actfunc)\n",
    "\n",
    "        #back propogate\n",
    "        delta_W2, delta_b2, delta_W1, delta_b1= backpropagation(h1,W2,output,y_t,X_t,derivfunc)\n",
    "        \n",
    "        #update weights using gradient descent\n",
    "        W2,b2,W1,b1=update_weights(alpha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1)\n",
    "\n",
    "        #again feed forward using updated weights\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_t,W1,b1,W2,b2,actfunc)\n",
    "\n",
    "        #Calculating Total Loss\n",
    "        loss=crossentropy(output,y_t)                 # put in training func later\n",
    "        print(\"Epoch {}: training loss = {}\".format(i + 1,loss))\n",
    "        listloss.append(loss)\n",
    "\n",
    "        #Retrieving the corresponding class labels:\n",
    "        labelpred=np.argmax(output,axis=1)\n",
    "        \n",
    "        #Training Accuracy:\n",
    "        accuracy = round((accuracy_score(labely_train, labelpred)*100),2)\n",
    "        print(\"Training Accuracy after Epoch {} : {}%\".format(i+1, accuracy))\n",
    "        li.append(accuracy)\n",
    "        \n",
    "    return W2,b2,W1,b1,li,listloss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(W2,b2,W1,b1,X_t,y_t,actfunc):\n",
    "    \n",
    "    dotprod1, h1, dotprod2, output=feedforward(X_t,W1,b1,W2,b2,actfunc)   \n",
    "    labely_test=y_t\n",
    "    labelpred=np.argmax(output,axis=1)\n",
    "    #accuracytest = round((accuracy_score(labely_test, labelpred)*100),2)        \n",
    "    #print(\"Accuracy on Testing data : {}%\".format(accuracytest))\n",
    "    return labely_test,labelpred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to measure accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labely_test, labelpred):\n",
    "    accuracytest = round((accuracy_score(labely_test, labelpred)*100),2)        \n",
    "    #print(\"Accuracy : {}%\".format(accuracytest))\n",
    "    return accuracytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration 1 :\n",
    "* Number of neurons = 64\n",
    "* Learning rate = 0.01\n",
    "* Iteratations =300\n",
    "* Activation Function : Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.4378994118199926\n",
      "Training Accuracy after Epoch 1 : 11.69%\n",
      "Epoch 2: training loss = 2.39796223947657\n",
      "Training Accuracy after Epoch 2 : 13.02%\n",
      "Epoch 3: training loss = 2.3608709309923737\n",
      "Training Accuracy after Epoch 3 : 14.41%\n",
      "Epoch 4: training loss = 2.3263803612582663\n",
      "Training Accuracy after Epoch 4 : 16.0%\n",
      "Epoch 5: training loss = 2.293933504367141\n",
      "Training Accuracy after Epoch 5 : 17.49%\n",
      "Epoch 6: training loss = 2.264095363280734\n",
      "Training Accuracy after Epoch 6 : 18.98%\n",
      "Epoch 7: training loss = 2.2359090863638778\n",
      "Training Accuracy after Epoch 7 : 20.56%\n",
      "Epoch 8: training loss = 2.2100257945067794\n",
      "Training Accuracy after Epoch 8 : 22.26%\n",
      "Epoch 9: training loss = 2.1856118785076277\n",
      "Training Accuracy after Epoch 9 : 23.66%\n",
      "Epoch 10: training loss = 2.1621966076521635\n",
      "Training Accuracy after Epoch 10 : 25.12%\n",
      "Epoch 11: training loss = 2.139804919051724\n",
      "Training Accuracy after Epoch 11 : 26.49%\n",
      "Epoch 12: training loss = 2.118926718621406\n",
      "Training Accuracy after Epoch 12 : 27.77%\n",
      "Epoch 13: training loss = 2.099458955803111\n",
      "Training Accuracy after Epoch 13 : 28.98%\n",
      "Epoch 14: training loss = 2.081247892220135\n",
      "Training Accuracy after Epoch 14 : 30.05%\n",
      "Epoch 15: training loss = 2.0637149688833265\n",
      "Training Accuracy after Epoch 15 : 31.07%\n",
      "Epoch 16: training loss = 2.0468614063105095\n",
      "Training Accuracy after Epoch 16 : 32.15%\n",
      "Epoch 17: training loss = 2.0309218818416386\n",
      "Training Accuracy after Epoch 17 : 33.23%\n",
      "Epoch 18: training loss = 2.0155972150381025\n",
      "Training Accuracy after Epoch 18 : 34.26%\n",
      "Epoch 19: training loss = 2.0002920358621874\n",
      "Training Accuracy after Epoch 19 : 35.16%\n",
      "Epoch 20: training loss = 1.9854455203045744\n",
      "Training Accuracy after Epoch 20 : 36.14%\n",
      "Epoch 21: training loss = 1.971028377273605\n",
      "Training Accuracy after Epoch 21 : 37.26%\n",
      "Epoch 22: training loss = 1.9570326830011342\n",
      "Training Accuracy after Epoch 22 : 38.38%\n",
      "Epoch 23: training loss = 1.943467573727989\n",
      "Training Accuracy after Epoch 23 : 39.4%\n",
      "Epoch 24: training loss = 1.9303206205653995\n",
      "Training Accuracy after Epoch 24 : 40.35%\n",
      "Epoch 25: training loss = 1.9173742915063705\n",
      "Training Accuracy after Epoch 25 : 41.45%\n",
      "Epoch 26: training loss = 1.9043317229941668\n",
      "Training Accuracy after Epoch 26 : 42.34%\n",
      "Epoch 27: training loss = 1.8912738918666985\n",
      "Training Accuracy after Epoch 27 : 43.35%\n",
      "Epoch 28: training loss = 1.8790394390517982\n",
      "Training Accuracy after Epoch 28 : 44.28%\n",
      "Epoch 29: training loss = 1.8673293700045754\n",
      "Training Accuracy after Epoch 29 : 45.16%\n",
      "Epoch 30: training loss = 1.8558252761787593\n",
      "Training Accuracy after Epoch 30 : 46.07%\n",
      "Epoch 31: training loss = 1.8446634458690734\n",
      "Training Accuracy after Epoch 31 : 46.99%\n",
      "Epoch 32: training loss = 1.8339169929151395\n",
      "Training Accuracy after Epoch 32 : 47.81%\n",
      "Epoch 33: training loss = 1.8233411919783113\n",
      "Training Accuracy after Epoch 33 : 48.5%\n",
      "Epoch 34: training loss = 1.8127760118794436\n",
      "Training Accuracy after Epoch 34 : 49.29%\n",
      "Epoch 35: training loss = 1.8024691639580817\n",
      "Training Accuracy after Epoch 35 : 50.05%\n",
      "Epoch 36: training loss = 1.7925013518307713\n",
      "Training Accuracy after Epoch 36 : 50.75%\n",
      "Epoch 37: training loss = 1.7828219853632885\n",
      "Training Accuracy after Epoch 37 : 51.48%\n",
      "Epoch 38: training loss = 1.7733186529930671\n",
      "Training Accuracy after Epoch 38 : 52.23%\n",
      "Epoch 39: training loss = 1.7638513251519108\n",
      "Training Accuracy after Epoch 39 : 52.88%\n",
      "Epoch 40: training loss = 1.7544814974613456\n",
      "Training Accuracy after Epoch 40 : 53.54%\n",
      "Epoch 41: training loss = 1.7453243521841904\n",
      "Training Accuracy after Epoch 41 : 54.25%\n",
      "Epoch 42: training loss = 1.7363060585430565\n",
      "Training Accuracy after Epoch 42 : 54.9%\n",
      "Epoch 43: training loss = 1.7275766655190756\n",
      "Training Accuracy after Epoch 43 : 55.49%\n",
      "Epoch 44: training loss = 1.7191005751728297\n",
      "Training Accuracy after Epoch 44 : 56.05%\n",
      "Epoch 45: training loss = 1.710715337843015\n",
      "Training Accuracy after Epoch 45 : 56.65%\n",
      "Epoch 46: training loss = 1.7023825171909366\n",
      "Training Accuracy after Epoch 46 : 57.2%\n",
      "Epoch 47: training loss = 1.6942604832203156\n",
      "Training Accuracy after Epoch 47 : 57.66%\n",
      "Epoch 48: training loss = 1.6862356823331652\n",
      "Training Accuracy after Epoch 48 : 58.2%\n",
      "Epoch 49: training loss = 1.6783242319436602\n",
      "Training Accuracy after Epoch 49 : 58.77%\n",
      "Epoch 50: training loss = 1.6707110008003871\n",
      "Training Accuracy after Epoch 50 : 59.27%\n",
      "Epoch 51: training loss = 1.663285664922046\n",
      "Training Accuracy after Epoch 51 : 59.77%\n",
      "Epoch 52: training loss = 1.655897699378601\n",
      "Training Accuracy after Epoch 52 : 60.14%\n",
      "Epoch 53: training loss = 1.6485358467701958\n",
      "Training Accuracy after Epoch 53 : 60.53%\n",
      "Epoch 54: training loss = 1.6411662778841853\n",
      "Training Accuracy after Epoch 54 : 60.9%\n",
      "Epoch 55: training loss = 1.6337936234200514\n",
      "Training Accuracy after Epoch 55 : 61.31%\n",
      "Epoch 56: training loss = 1.6264212509987006\n",
      "Training Accuracy after Epoch 56 : 61.69%\n",
      "Epoch 57: training loss = 1.6190886874690804\n",
      "Training Accuracy after Epoch 57 : 62.07%\n",
      "Epoch 58: training loss = 1.611859248229327\n",
      "Training Accuracy after Epoch 58 : 62.5%\n",
      "Epoch 59: training loss = 1.6047334003975835\n",
      "Training Accuracy after Epoch 59 : 62.83%\n",
      "Epoch 60: training loss = 1.5977460457317756\n",
      "Training Accuracy after Epoch 60 : 63.15%\n",
      "Epoch 61: training loss = 1.5908849138474974\n",
      "Training Accuracy after Epoch 61 : 63.56%\n",
      "Epoch 62: training loss = 1.5841168141154984\n",
      "Training Accuracy after Epoch 62 : 63.85%\n",
      "Epoch 63: training loss = 1.5774485613694889\n",
      "Training Accuracy after Epoch 63 : 64.16%\n",
      "Epoch 64: training loss = 1.5708223865075623\n",
      "Training Accuracy after Epoch 64 : 64.43%\n",
      "Epoch 65: training loss = 1.5642504384223157\n",
      "Training Accuracy after Epoch 65 : 64.57%\n",
      "Epoch 66: training loss = 1.5578577106076381\n",
      "Training Accuracy after Epoch 66 : 64.79%\n",
      "Epoch 67: training loss = 1.55160153995815\n",
      "Training Accuracy after Epoch 67 : 65.1%\n",
      "Epoch 68: training loss = 1.5454705119515837\n",
      "Training Accuracy after Epoch 68 : 65.41%\n",
      "Epoch 69: training loss = 1.5394983693158328\n",
      "Training Accuracy after Epoch 69 : 65.68%\n",
      "Epoch 70: training loss = 1.5336372099026065\n",
      "Training Accuracy after Epoch 70 : 65.98%\n",
      "Epoch 71: training loss = 1.527844449869216\n",
      "Training Accuracy after Epoch 71 : 66.24%\n",
      "Epoch 72: training loss = 1.5221131799302356\n",
      "Training Accuracy after Epoch 72 : 66.52%\n",
      "Epoch 73: training loss = 1.5164317563768466\n",
      "Training Accuracy after Epoch 73 : 66.81%\n",
      "Epoch 74: training loss = 1.5108502656039164\n",
      "Training Accuracy after Epoch 74 : 67.01%\n",
      "Epoch 75: training loss = 1.5053527244692644\n",
      "Training Accuracy after Epoch 75 : 67.25%\n",
      "Epoch 76: training loss = 1.4998734519930967\n",
      "Training Accuracy after Epoch 76 : 67.44%\n",
      "Epoch 77: training loss = 1.4943890502598418\n",
      "Training Accuracy after Epoch 77 : 67.65%\n",
      "Epoch 78: training loss = 1.4889395086493558\n",
      "Training Accuracy after Epoch 78 : 67.9%\n",
      "Epoch 79: training loss = 1.4835335608340066\n",
      "Training Accuracy after Epoch 79 : 68.12%\n",
      "Epoch 80: training loss = 1.4782034029840603\n",
      "Training Accuracy after Epoch 80 : 68.35%\n",
      "Epoch 81: training loss = 1.4729409777283011\n",
      "Training Accuracy after Epoch 81 : 68.53%\n",
      "Epoch 82: training loss = 1.4677435423405967\n",
      "Training Accuracy after Epoch 82 : 68.77%\n",
      "Epoch 83: training loss = 1.462616478685291\n",
      "Training Accuracy after Epoch 83 : 68.98%\n",
      "Epoch 84: training loss = 1.457537707326957\n",
      "Training Accuracy after Epoch 84 : 69.19%\n",
      "Epoch 85: training loss = 1.4525137321738086\n",
      "Training Accuracy after Epoch 85 : 69.36%\n",
      "Epoch 86: training loss = 1.4475510818429425\n",
      "Training Accuracy after Epoch 86 : 69.53%\n",
      "Epoch 87: training loss = 1.4426070895530991\n",
      "Training Accuracy after Epoch 87 : 69.7%\n",
      "Epoch 88: training loss = 1.437663016961586\n",
      "Training Accuracy after Epoch 88 : 69.9%\n",
      "Epoch 89: training loss = 1.4327384403138295\n",
      "Training Accuracy after Epoch 89 : 70.05%\n",
      "Epoch 90: training loss = 1.427836769950959\n",
      "Training Accuracy after Epoch 90 : 70.25%\n",
      "Epoch 91: training loss = 1.4229859147702713\n",
      "Training Accuracy after Epoch 91 : 70.41%\n",
      "Epoch 92: training loss = 1.4181864240688198\n",
      "Training Accuracy after Epoch 92 : 70.6%\n",
      "Epoch 93: training loss = 1.413450357833555\n",
      "Training Accuracy after Epoch 93 : 70.79%\n",
      "Epoch 94: training loss = 1.4088113855902464\n",
      "Training Accuracy after Epoch 94 : 70.94%\n",
      "Epoch 95: training loss = 1.40424945351152\n",
      "Training Accuracy after Epoch 95 : 71.15%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: training loss = 1.399743821991576\n",
      "Training Accuracy after Epoch 96 : 71.27%\n",
      "Epoch 97: training loss = 1.3952748228409115\n",
      "Training Accuracy after Epoch 97 : 71.41%\n",
      "Epoch 98: training loss = 1.3908068806693468\n",
      "Training Accuracy after Epoch 98 : 71.57%\n",
      "Epoch 99: training loss = 1.3863149916275113\n",
      "Training Accuracy after Epoch 99 : 71.71%\n",
      "Epoch 100: training loss = 1.3817720116538235\n",
      "Training Accuracy after Epoch 100 : 71.9%\n",
      "Epoch 101: training loss = 1.3772232172798455\n",
      "Training Accuracy after Epoch 101 : 72.0%\n",
      "Epoch 102: training loss = 1.3727813638199882\n",
      "Training Accuracy after Epoch 102 : 72.12%\n",
      "Epoch 103: training loss = 1.3684440705364838\n",
      "Training Accuracy after Epoch 103 : 72.26%\n",
      "Epoch 104: training loss = 1.3641731384675737\n",
      "Training Accuracy after Epoch 104 : 72.41%\n",
      "Epoch 105: training loss = 1.3599384373062522\n",
      "Training Accuracy after Epoch 105 : 72.54%\n",
      "Epoch 106: training loss = 1.355717306229669\n",
      "Training Accuracy after Epoch 106 : 72.68%\n",
      "Epoch 107: training loss = 1.3515143717607052\n",
      "Training Accuracy after Epoch 107 : 72.79%\n",
      "Epoch 108: training loss = 1.34734972648667\n",
      "Training Accuracy after Epoch 108 : 72.96%\n",
      "Epoch 109: training loss = 1.343212901287605\n",
      "Training Accuracy after Epoch 109 : 73.15%\n",
      "Epoch 110: training loss = 1.3390941212266\n",
      "Training Accuracy after Epoch 110 : 73.29%\n",
      "Epoch 111: training loss = 1.3350046826386281\n",
      "Training Accuracy after Epoch 111 : 73.42%\n",
      "Epoch 112: training loss = 1.330939463890289\n",
      "Training Accuracy after Epoch 112 : 73.51%\n",
      "Epoch 113: training loss = 1.32688445054236\n",
      "Training Accuracy after Epoch 113 : 73.66%\n",
      "Epoch 114: training loss = 1.322845127833339\n",
      "Training Accuracy after Epoch 114 : 73.78%\n",
      "Epoch 115: training loss = 1.318844414375007\n",
      "Training Accuracy after Epoch 115 : 73.9%\n",
      "Epoch 116: training loss = 1.314878463957501\n",
      "Training Accuracy after Epoch 116 : 74.07%\n",
      "Epoch 117: training loss = 1.3109226033389303\n",
      "Training Accuracy after Epoch 117 : 74.17%\n",
      "Epoch 118: training loss = 1.3069790162892116\n",
      "Training Accuracy after Epoch 118 : 74.33%\n",
      "Epoch 119: training loss = 1.3030578837447222\n",
      "Training Accuracy after Epoch 119 : 74.45%\n",
      "Epoch 120: training loss = 1.299153301748132\n",
      "Training Accuracy after Epoch 120 : 74.55%\n",
      "Epoch 121: training loss = 1.2952607664498763\n",
      "Training Accuracy after Epoch 121 : 74.67%\n",
      "Epoch 122: training loss = 1.2913668315875517\n",
      "Training Accuracy after Epoch 122 : 74.75%\n",
      "Epoch 123: training loss = 1.2875288516729684\n",
      "Training Accuracy after Epoch 123 : 74.88%\n",
      "Epoch 124: training loss = 1.283723309611249\n",
      "Training Accuracy after Epoch 124 : 74.97%\n",
      "Epoch 125: training loss = 1.2799223568688767\n",
      "Training Accuracy after Epoch 125 : 75.09%\n",
      "Epoch 126: training loss = 1.2761377219053345\n",
      "Training Accuracy after Epoch 126 : 75.18%\n",
      "Epoch 127: training loss = 1.272378006482973\n",
      "Training Accuracy after Epoch 127 : 75.29%\n",
      "Epoch 128: training loss = 1.2686571536645357\n",
      "Training Accuracy after Epoch 128 : 75.38%\n",
      "Epoch 129: training loss = 1.2649511995749008\n",
      "Training Accuracy after Epoch 129 : 75.47%\n",
      "Epoch 130: training loss = 1.2612539468774988\n",
      "Training Accuracy after Epoch 130 : 75.58%\n",
      "Epoch 131: training loss = 1.2575837143356192\n",
      "Training Accuracy after Epoch 131 : 75.72%\n",
      "Epoch 132: training loss = 1.2539572576473865\n",
      "Training Accuracy after Epoch 132 : 75.84%\n",
      "Epoch 133: training loss = 1.2503562784088675\n",
      "Training Accuracy after Epoch 133 : 75.96%\n",
      "Epoch 134: training loss = 1.2467903176916375\n",
      "Training Accuracy after Epoch 134 : 76.05%\n",
      "Epoch 135: training loss = 1.2432660277932766\n",
      "Training Accuracy after Epoch 135 : 76.17%\n",
      "Epoch 136: training loss = 1.2397676010095888\n",
      "Training Accuracy after Epoch 136 : 76.26%\n",
      "Epoch 137: training loss = 1.2362711620016387\n",
      "Training Accuracy after Epoch 137 : 76.38%\n",
      "Epoch 138: training loss = 1.232755312486136\n",
      "Training Accuracy after Epoch 138 : 76.49%\n",
      "Epoch 139: training loss = 1.229244084755971\n",
      "Training Accuracy after Epoch 139 : 76.6%\n",
      "Epoch 140: training loss = 1.2257667707226667\n",
      "Training Accuracy after Epoch 140 : 76.68%\n",
      "Epoch 141: training loss = 1.222362083533703\n",
      "Training Accuracy after Epoch 141 : 76.81%\n",
      "Epoch 142: training loss = 1.219015772372857\n",
      "Training Accuracy after Epoch 142 : 76.89%\n",
      "Epoch 143: training loss = 1.2156836389894772\n",
      "Training Accuracy after Epoch 143 : 76.98%\n",
      "Epoch 144: training loss = 1.2123647710502745\n",
      "Training Accuracy after Epoch 144 : 77.07%\n",
      "Epoch 145: training loss = 1.2090980397352782\n",
      "Training Accuracy after Epoch 145 : 77.13%\n",
      "Epoch 146: training loss = 1.2058932301442251\n",
      "Training Accuracy after Epoch 146 : 77.22%\n",
      "Epoch 147: training loss = 1.2027099066098677\n",
      "Training Accuracy after Epoch 147 : 77.3%\n",
      "Epoch 148: training loss = 1.1995245949107887\n",
      "Training Accuracy after Epoch 148 : 77.38%\n",
      "Epoch 149: training loss = 1.1963326479878411\n",
      "Training Accuracy after Epoch 149 : 77.44%\n",
      "Epoch 150: training loss = 1.1931204113644205\n",
      "Training Accuracy after Epoch 150 : 77.53%\n",
      "Epoch 151: training loss = 1.1898923794776635\n",
      "Training Accuracy after Epoch 151 : 77.63%\n",
      "Epoch 152: training loss = 1.1866976044130455\n",
      "Training Accuracy after Epoch 152 : 77.71%\n",
      "Epoch 153: training loss = 1.183535075287218\n",
      "Training Accuracy after Epoch 153 : 77.77%\n",
      "Epoch 154: training loss = 1.1803843660273428\n",
      "Training Accuracy after Epoch 154 : 77.84%\n",
      "Epoch 155: training loss = 1.1772552539313486\n",
      "Training Accuracy after Epoch 155 : 77.9%\n",
      "Epoch 156: training loss = 1.1741597372007528\n",
      "Training Accuracy after Epoch 156 : 77.96%\n",
      "Epoch 157: training loss = 1.1710715474998348\n",
      "Training Accuracy after Epoch 157 : 78.03%\n",
      "Epoch 158: training loss = 1.1679989421705594\n",
      "Training Accuracy after Epoch 158 : 78.11%\n",
      "Epoch 159: training loss = 1.1649528297160332\n",
      "Training Accuracy after Epoch 159 : 78.16%\n",
      "Epoch 160: training loss = 1.1618934339387674\n",
      "Training Accuracy after Epoch 160 : 78.24%\n",
      "Epoch 161: training loss = 1.1587953771944992\n",
      "Training Accuracy after Epoch 161 : 78.27%\n",
      "Epoch 162: training loss = 1.1557456757263957\n",
      "Training Accuracy after Epoch 162 : 78.37%\n",
      "Epoch 163: training loss = 1.152768645794562\n",
      "Training Accuracy after Epoch 163 : 78.46%\n",
      "Epoch 164: training loss = 1.1498260221382384\n",
      "Training Accuracy after Epoch 164 : 78.53%\n",
      "Epoch 165: training loss = 1.1469240583444331\n",
      "Training Accuracy after Epoch 165 : 78.63%\n",
      "Epoch 166: training loss = 1.1440322959851088\n",
      "Training Accuracy after Epoch 166 : 78.7%\n",
      "Epoch 167: training loss = 1.1411332567722297\n",
      "Training Accuracy after Epoch 167 : 78.76%\n",
      "Epoch 168: training loss = 1.1382584865280845\n",
      "Training Accuracy after Epoch 168 : 78.83%\n",
      "Epoch 169: training loss = 1.1354482294993244\n",
      "Training Accuracy after Epoch 169 : 78.91%\n",
      "Epoch 170: training loss = 1.1326929680660627\n",
      "Training Accuracy after Epoch 170 : 78.97%\n",
      "Epoch 171: training loss = 1.1299594428122695\n",
      "Training Accuracy after Epoch 171 : 79.02%\n",
      "Epoch 172: training loss = 1.1272342840774405\n",
      "Training Accuracy after Epoch 172 : 79.12%\n",
      "Epoch 173: training loss = 1.1245078442018035\n",
      "Training Accuracy after Epoch 173 : 79.18%\n",
      "Epoch 174: training loss = 1.1217707755473856\n",
      "Training Accuracy after Epoch 174 : 79.2%\n",
      "Epoch 175: training loss = 1.1190250399007844\n",
      "Training Accuracy after Epoch 175 : 79.25%\n",
      "Epoch 176: training loss = 1.116308333113082\n",
      "Training Accuracy after Epoch 176 : 79.32%\n",
      "Epoch 177: training loss = 1.1136268261917102\n",
      "Training Accuracy after Epoch 177 : 79.39%\n",
      "Epoch 178: training loss = 1.1109607848488239\n",
      "Training Accuracy after Epoch 178 : 79.44%\n",
      "Epoch 179: training loss = 1.1083124662250903\n",
      "Training Accuracy after Epoch 179 : 79.51%\n",
      "Epoch 180: training loss = 1.1056753858671784\n",
      "Training Accuracy after Epoch 180 : 79.56%\n",
      "Epoch 181: training loss = 1.1030431286447777\n",
      "Training Accuracy after Epoch 181 : 79.6%\n",
      "Epoch 182: training loss = 1.1004202862828072\n",
      "Training Accuracy after Epoch 182 : 79.68%\n",
      "Epoch 183: training loss = 1.097829867073574\n",
      "Training Accuracy after Epoch 183 : 79.72%\n",
      "Epoch 184: training loss = 1.0952835420662332\n",
      "Training Accuracy after Epoch 184 : 79.8%\n",
      "Epoch 185: training loss = 1.092765130371243\n",
      "Training Accuracy after Epoch 185 : 79.83%\n",
      "Epoch 186: training loss = 1.0902719325427606\n",
      "Training Accuracy after Epoch 186 : 79.88%\n",
      "Epoch 187: training loss = 1.0877804639492925\n",
      "Training Accuracy after Epoch 187 : 79.95%\n",
      "Epoch 188: training loss = 1.085268770960291\n",
      "Training Accuracy after Epoch 188 : 79.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189: training loss = 1.0827531309820302\n",
      "Training Accuracy after Epoch 189 : 80.04%\n",
      "Epoch 190: training loss = 1.0802516002031763\n",
      "Training Accuracy after Epoch 190 : 80.07%\n",
      "Epoch 191: training loss = 1.077777209317196\n",
      "Training Accuracy after Epoch 191 : 80.09%\n",
      "Epoch 192: training loss = 1.075324019818414\n",
      "Training Accuracy after Epoch 192 : 80.15%\n",
      "Epoch 193: training loss = 1.072878639709743\n",
      "Training Accuracy after Epoch 193 : 80.21%\n",
      "Epoch 194: training loss = 1.070439822907809\n",
      "Training Accuracy after Epoch 194 : 80.25%\n",
      "Epoch 195: training loss = 1.0680252000759538\n",
      "Training Accuracy after Epoch 195 : 80.31%\n",
      "Epoch 196: training loss = 1.0656451053853324\n",
      "Training Accuracy after Epoch 196 : 80.38%\n",
      "Epoch 197: training loss = 1.0632925200438952\n",
      "Training Accuracy after Epoch 197 : 80.46%\n",
      "Epoch 198: training loss = 1.0609610618184382\n",
      "Training Accuracy after Epoch 198 : 80.53%\n",
      "Epoch 199: training loss = 1.0586443243490982\n",
      "Training Accuracy after Epoch 199 : 80.57%\n",
      "Epoch 200: training loss = 1.0563354740459143\n",
      "Training Accuracy after Epoch 200 : 80.65%\n",
      "Epoch 201: training loss = 1.0540385204911864\n",
      "Training Accuracy after Epoch 201 : 80.69%\n",
      "Epoch 202: training loss = 1.051762290852859\n",
      "Training Accuracy after Epoch 202 : 80.76%\n",
      "Epoch 203: training loss = 1.0495031892688957\n",
      "Training Accuracy after Epoch 203 : 80.8%\n",
      "Epoch 204: training loss = 1.0472521951907139\n",
      "Training Accuracy after Epoch 204 : 80.84%\n",
      "Epoch 205: training loss = 1.0450040963616398\n",
      "Training Accuracy after Epoch 205 : 80.88%\n",
      "Epoch 206: training loss = 1.0427618547128503\n",
      "Training Accuracy after Epoch 206 : 80.92%\n",
      "Epoch 207: training loss = 1.0405294143985726\n",
      "Training Accuracy after Epoch 207 : 80.96%\n",
      "Epoch 208: training loss = 1.0383091881618196\n",
      "Training Accuracy after Epoch 208 : 81.04%\n",
      "Epoch 209: training loss = 1.0361018317217636\n",
      "Training Accuracy after Epoch 209 : 81.09%\n",
      "Epoch 210: training loss = 1.0339110525828903\n",
      "Training Accuracy after Epoch 210 : 81.13%\n",
      "Epoch 211: training loss = 1.0317400639888212\n",
      "Training Accuracy after Epoch 211 : 81.15%\n",
      "Epoch 212: training loss = 1.029588138334855\n",
      "Training Accuracy after Epoch 212 : 81.23%\n",
      "Epoch 213: training loss = 1.0274541695331545\n",
      "Training Accuracy after Epoch 213 : 81.3%\n",
      "Epoch 214: training loss = 1.0253378177467956\n",
      "Training Accuracy after Epoch 214 : 81.36%\n",
      "Epoch 215: training loss = 1.0232390942637246\n",
      "Training Accuracy after Epoch 215 : 81.4%\n",
      "Epoch 216: training loss = 1.021158641761989\n",
      "Training Accuracy after Epoch 216 : 81.44%\n",
      "Epoch 217: training loss = 1.0190961232166316\n",
      "Training Accuracy after Epoch 217 : 81.5%\n",
      "Epoch 218: training loss = 1.0170483709262506\n",
      "Training Accuracy after Epoch 218 : 81.53%\n",
      "Epoch 219: training loss = 1.0150119112397904\n",
      "Training Accuracy after Epoch 219 : 81.56%\n",
      "Epoch 220: training loss = 1.012985119370808\n",
      "Training Accuracy after Epoch 220 : 81.61%\n",
      "Epoch 221: training loss = 1.0109662977010156\n",
      "Training Accuracy after Epoch 221 : 81.65%\n",
      "Epoch 222: training loss = 1.0089542794981055\n",
      "Training Accuracy after Epoch 222 : 81.69%\n",
      "Epoch 223: training loss = 1.006955418891366\n",
      "Training Accuracy after Epoch 223 : 81.73%\n",
      "Epoch 224: training loss = 1.004975187319023\n",
      "Training Accuracy after Epoch 224 : 81.77%\n",
      "Epoch 225: training loss = 1.0030096006635851\n",
      "Training Accuracy after Epoch 225 : 81.85%\n",
      "Epoch 226: training loss = 1.0010532518350983\n",
      "Training Accuracy after Epoch 226 : 81.89%\n",
      "Epoch 227: training loss = 0.9991022800808553\n",
      "Training Accuracy after Epoch 227 : 81.94%\n",
      "Epoch 228: training loss = 0.9971533883297752\n",
      "Training Accuracy after Epoch 228 : 81.96%\n",
      "Epoch 229: training loss = 0.9952063399871073\n",
      "Training Accuracy after Epoch 229 : 82.01%\n",
      "Epoch 230: training loss = 0.9932618204769512\n",
      "Training Accuracy after Epoch 230 : 82.04%\n",
      "Epoch 231: training loss = 0.9913222108689785\n",
      "Training Accuracy after Epoch 231 : 82.08%\n",
      "Epoch 232: training loss = 0.9893975045639233\n",
      "Training Accuracy after Epoch 232 : 82.11%\n",
      "Epoch 233: training loss = 0.9874845379855308\n",
      "Training Accuracy after Epoch 233 : 82.14%\n",
      "Epoch 234: training loss = 0.9855801474452218\n",
      "Training Accuracy after Epoch 234 : 82.15%\n",
      "Epoch 235: training loss = 0.9836823833771139\n",
      "Training Accuracy after Epoch 235 : 82.2%\n",
      "Epoch 236: training loss = 0.9817891201866028\n",
      "Training Accuracy after Epoch 236 : 82.27%\n",
      "Epoch 237: training loss = 0.9799013588875304\n",
      "Training Accuracy after Epoch 237 : 82.3%\n",
      "Epoch 238: training loss = 0.9780208865547164\n",
      "Training Accuracy after Epoch 238 : 82.33%\n",
      "Epoch 239: training loss = 0.9761484959473892\n",
      "Training Accuracy after Epoch 239 : 82.38%\n",
      "Epoch 240: training loss = 0.9742915122408887\n",
      "Training Accuracy after Epoch 240 : 82.41%\n",
      "Epoch 241: training loss = 0.9724561604591685\n",
      "Training Accuracy after Epoch 241 : 82.47%\n",
      "Epoch 242: training loss = 0.9706389508023955\n",
      "Training Accuracy after Epoch 242 : 82.52%\n",
      "Epoch 243: training loss = 0.9688360180278998\n",
      "Training Accuracy after Epoch 243 : 82.56%\n",
      "Epoch 244: training loss = 0.9670443688665565\n",
      "Training Accuracy after Epoch 244 : 82.59%\n",
      "Epoch 245: training loss = 0.9652619105807596\n",
      "Training Accuracy after Epoch 245 : 82.66%\n",
      "Epoch 246: training loss = 0.9634868690854755\n",
      "Training Accuracy after Epoch 246 : 82.68%\n",
      "Epoch 247: training loss = 0.9617175001288022\n",
      "Training Accuracy after Epoch 247 : 82.72%\n",
      "Epoch 248: training loss = 0.9599540487011106\n",
      "Training Accuracy after Epoch 248 : 82.76%\n",
      "Epoch 249: training loss = 0.9581959366357857\n",
      "Training Accuracy after Epoch 249 : 82.77%\n",
      "Epoch 250: training loss = 0.9564410217066647\n",
      "Training Accuracy after Epoch 250 : 82.79%\n",
      "Epoch 251: training loss = 0.9546835888050607\n",
      "Training Accuracy after Epoch 251 : 82.83%\n",
      "Epoch 252: training loss = 0.9529152922384732\n",
      "Training Accuracy after Epoch 252 : 82.85%\n",
      "Epoch 253: training loss = 0.951145509638905\n",
      "Training Accuracy after Epoch 253 : 82.88%\n",
      "Epoch 254: training loss = 0.94939987111353\n",
      "Training Accuracy after Epoch 254 : 82.92%\n",
      "Epoch 255: training loss = 0.9476750597947884\n",
      "Training Accuracy after Epoch 255 : 82.95%\n",
      "Epoch 256: training loss = 0.9459531737402748\n",
      "Training Accuracy after Epoch 256 : 82.99%\n",
      "Epoch 257: training loss = 0.9442350387220689\n",
      "Training Accuracy after Epoch 257 : 83.01%\n",
      "Epoch 258: training loss = 0.9425316332373791\n",
      "Training Accuracy after Epoch 258 : 83.02%\n",
      "Epoch 259: training loss = 0.9408432474530868\n",
      "Training Accuracy after Epoch 259 : 83.05%\n",
      "Epoch 260: training loss = 0.9391625476140519\n",
      "Training Accuracy after Epoch 260 : 83.06%\n",
      "Epoch 261: training loss = 0.9374837022115441\n",
      "Training Accuracy after Epoch 261 : 83.08%\n",
      "Epoch 262: training loss = 0.9358109557452086\n",
      "Training Accuracy after Epoch 262 : 83.13%\n",
      "Epoch 263: training loss = 0.9341476297167164\n",
      "Training Accuracy after Epoch 263 : 83.18%\n",
      "Epoch 264: training loss = 0.932492087461774\n",
      "Training Accuracy after Epoch 264 : 83.19%\n",
      "Epoch 265: training loss = 0.9308429193195263\n",
      "Training Accuracy after Epoch 265 : 83.23%\n",
      "Epoch 266: training loss = 0.9291998903249071\n",
      "Training Accuracy after Epoch 266 : 83.26%\n",
      "Epoch 267: training loss = 0.9275642599214998\n",
      "Training Accuracy after Epoch 267 : 83.3%\n",
      "Epoch 268: training loss = 0.9259356194185698\n",
      "Training Accuracy after Epoch 268 : 83.34%\n",
      "Epoch 269: training loss = 0.9243103405489734\n",
      "Training Accuracy after Epoch 269 : 83.35%\n",
      "Epoch 270: training loss = 0.922685289219518\n",
      "Training Accuracy after Epoch 270 : 83.39%\n",
      "Epoch 271: training loss = 0.9210626108738776\n",
      "Training Accuracy after Epoch 271 : 83.42%\n",
      "Epoch 272: training loss = 0.9194471663945691\n",
      "Training Accuracy after Epoch 272 : 83.46%\n",
      "Epoch 273: training loss = 0.917836700050006\n",
      "Training Accuracy after Epoch 273 : 83.47%\n",
      "Epoch 274: training loss = 0.9162226023372253\n",
      "Training Accuracy after Epoch 274 : 83.5%\n",
      "Epoch 275: training loss = 0.9146049267389657\n",
      "Training Accuracy after Epoch 275 : 83.53%\n",
      "Epoch 276: training loss = 0.9129876208276981\n",
      "Training Accuracy after Epoch 276 : 83.55%\n",
      "Epoch 277: training loss = 0.9113858446528248\n",
      "Training Accuracy after Epoch 277 : 83.59%\n",
      "Epoch 278: training loss = 0.9098080034460029\n",
      "Training Accuracy after Epoch 278 : 83.63%\n",
      "Epoch 279: training loss = 0.9082515029027863\n",
      "Training Accuracy after Epoch 279 : 83.65%\n",
      "Epoch 280: training loss = 0.9067062503100074\n",
      "Training Accuracy after Epoch 280 : 83.68%\n",
      "Epoch 281: training loss = 0.9051591281850252\n",
      "Training Accuracy after Epoch 281 : 83.71%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282: training loss = 0.9036024882780794\n",
      "Training Accuracy after Epoch 282 : 83.75%\n",
      "Epoch 283: training loss = 0.9020528144576964\n",
      "Training Accuracy after Epoch 283 : 83.77%\n",
      "Epoch 284: training loss = 0.9005161187548951\n",
      "Training Accuracy after Epoch 284 : 83.79%\n",
      "Epoch 285: training loss = 0.8989769162778344\n",
      "Training Accuracy after Epoch 285 : 83.81%\n",
      "Epoch 286: training loss = 0.8974375685152366\n",
      "Training Accuracy after Epoch 286 : 83.84%\n",
      "Epoch 287: training loss = 0.89591404514937\n",
      "Training Accuracy after Epoch 287 : 83.87%\n",
      "Epoch 288: training loss = 0.8944115205031341\n",
      "Training Accuracy after Epoch 288 : 83.89%\n",
      "Epoch 289: training loss = 0.8929194157161293\n",
      "Training Accuracy after Epoch 289 : 83.93%\n",
      "Epoch 290: training loss = 0.8914303195031447\n",
      "Training Accuracy after Epoch 290 : 83.98%\n",
      "Epoch 291: training loss = 0.8899416942338635\n",
      "Training Accuracy after Epoch 291 : 84.0%\n",
      "Epoch 292: training loss = 0.8884600339964606\n",
      "Training Accuracy after Epoch 292 : 84.01%\n",
      "Epoch 293: training loss = 0.8869930645161516\n",
      "Training Accuracy after Epoch 293 : 84.05%\n",
      "Epoch 294: training loss = 0.8855393262026063\n",
      "Training Accuracy after Epoch 294 : 84.08%\n",
      "Epoch 295: training loss = 0.884096102636568\n",
      "Training Accuracy after Epoch 295 : 84.11%\n",
      "Epoch 296: training loss = 0.8826619734757917\n",
      "Training Accuracy after Epoch 296 : 84.13%\n",
      "Epoch 297: training loss = 0.8812357305279903\n",
      "Training Accuracy after Epoch 297 : 84.14%\n",
      "Epoch 298: training loss = 0.879815782931391\n",
      "Training Accuracy after Epoch 298 : 84.16%\n",
      "Epoch 299: training loss = 0.8784001417305362\n",
      "Training Accuracy after Epoch 299 : 84.18%\n",
      "Epoch 300: training loss = 0.876987143424982\n",
      "Training Accuracy after Epoch 300 : 84.2%\n"
     ]
    }
   ],
   "source": [
    "W2,b2,W1,b1,li1,loss1=training(X_trainval,y_trainval,64,0.01,300,sigmoid,sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loss using Configuration 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe8049f348>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "loss= loss1\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Test Loss using configuration 1', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "#plt.scatter(epochs,loss,alpha=1,c='green')\n",
    "plt.plot(epochs,loss,c='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training accuracies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe80732e08>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "accuracies= li1\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Training Accuracies using configuration 1', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(epochs,accuracies,alpha=1,c='green')\n",
    "plt.plot(epochs,accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Validation data using configuration 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Validation data using configuration 1: 83.43%\n"
     ]
    }
   ],
   "source": [
    "labely_test,labelpred=testing(W2,b2,W1,b1,X_testval,y_testval,sigmoid)\n",
    "accuracy_val=accuracy(labely_test, labelpred)\n",
    "print(\"Accuracy on Validation data using configuration 1: {}%\".format(accuracy_val))\n",
    "acc1=accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration 2 :\n",
    "* Number of neurons = 64\n",
    "* Learning rate = 0.05\n",
    "* Iteratations =300\n",
    "* Activation Function : Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.2669175230933987\n",
      "Training Accuracy after Epoch 1 : 13.08%\n",
      "Epoch 2: training loss = 2.1308644087006114\n",
      "Training Accuracy after Epoch 2 : 23.09%\n",
      "Epoch 3: training loss = 2.0226069858501172\n",
      "Training Accuracy after Epoch 3 : 34.11%\n",
      "Epoch 4: training loss = 1.9340098933622039\n",
      "Training Accuracy after Epoch 4 : 42.02%\n",
      "Epoch 5: training loss = 1.8588591915049013\n",
      "Training Accuracy after Epoch 5 : 47.48%\n",
      "Epoch 6: training loss = 1.793099183835952\n",
      "Training Accuracy after Epoch 6 : 51.83%\n",
      "Epoch 7: training loss = 1.7347867526086567\n",
      "Training Accuracy after Epoch 7 : 55.45%\n",
      "Epoch 8: training loss = 1.681205083399042\n",
      "Training Accuracy after Epoch 8 : 58.58%\n",
      "Epoch 9: training loss = 1.6343648029775286\n",
      "Training Accuracy after Epoch 9 : 60.99%\n",
      "Epoch 10: training loss = 1.5936809986062082\n",
      "Training Accuracy after Epoch 10 : 63.01%\n",
      "Epoch 11: training loss = 1.5552144976453752\n",
      "Training Accuracy after Epoch 11 : 64.73%\n",
      "Epoch 12: training loss = 1.5188919889692516\n",
      "Training Accuracy after Epoch 12 : 66.74%\n",
      "Epoch 13: training loss = 1.4841926503176348\n",
      "Training Accuracy after Epoch 13 : 68.25%\n",
      "Epoch 14: training loss = 1.45017639596746\n",
      "Training Accuracy after Epoch 14 : 69.8%\n",
      "Epoch 15: training loss = 1.4185573568557113\n",
      "Training Accuracy after Epoch 15 : 70.9%\n",
      "Epoch 16: training loss = 1.3895202368486568\n",
      "Training Accuracy after Epoch 16 : 71.97%\n",
      "Epoch 17: training loss = 1.3628887079796326\n",
      "Training Accuracy after Epoch 17 : 72.48%\n",
      "Epoch 18: training loss = 1.33687213455454\n",
      "Training Accuracy after Epoch 18 : 73.48%\n",
      "Epoch 19: training loss = 1.3119744363394974\n",
      "Training Accuracy after Epoch 19 : 74.03%\n",
      "Epoch 20: training loss = 1.2879893117568537\n",
      "Training Accuracy after Epoch 20 : 74.86%\n",
      "Epoch 21: training loss = 1.265329230296261\n",
      "Training Accuracy after Epoch 21 : 75.46%\n",
      "Epoch 22: training loss = 1.2440739251734623\n",
      "Training Accuracy after Epoch 22 : 76.08%\n",
      "Epoch 23: training loss = 1.2242317873134319\n",
      "Training Accuracy after Epoch 23 : 76.31%\n",
      "Epoch 24: training loss = 1.2044597521182945\n",
      "Training Accuracy after Epoch 24 : 76.88%\n",
      "Epoch 25: training loss = 1.1853299908094201\n",
      "Training Accuracy after Epoch 25 : 77.22%\n",
      "Epoch 26: training loss = 1.1680905324467168\n",
      "Training Accuracy after Epoch 26 : 77.86%\n",
      "Epoch 27: training loss = 1.1516754697865945\n",
      "Training Accuracy after Epoch 27 : 78.13%\n",
      "Epoch 28: training loss = 1.1358705309827328\n",
      "Training Accuracy after Epoch 28 : 78.7%\n",
      "Epoch 29: training loss = 1.1211089245454615\n",
      "Training Accuracy after Epoch 29 : 78.77%\n",
      "Epoch 30: training loss = 1.1066510328030768\n",
      "Training Accuracy after Epoch 30 : 79.16%\n",
      "Epoch 31: training loss = 1.0927765778760956\n",
      "Training Accuracy after Epoch 31 : 79.32%\n",
      "Epoch 32: training loss = 1.0796186053535635\n",
      "Training Accuracy after Epoch 32 : 79.77%\n",
      "Epoch 33: training loss = 1.066849299758851\n",
      "Training Accuracy after Epoch 33 : 79.85%\n",
      "Epoch 34: training loss = 1.054932149754288\n",
      "Training Accuracy after Epoch 34 : 80.14%\n",
      "Epoch 35: training loss = 1.0433421768598712\n",
      "Training Accuracy after Epoch 35 : 80.27%\n",
      "Epoch 36: training loss = 1.0321348099157448\n",
      "Training Accuracy after Epoch 36 : 80.69%\n",
      "Epoch 37: training loss = 1.0210895304886949\n",
      "Training Accuracy after Epoch 37 : 80.85%\n",
      "Epoch 38: training loss = 1.010533947180446\n",
      "Training Accuracy after Epoch 38 : 81.15%\n",
      "Epoch 39: training loss = 0.9999585090406793\n",
      "Training Accuracy after Epoch 39 : 81.28%\n",
      "Epoch 40: training loss = 0.9898790090294275\n",
      "Training Accuracy after Epoch 40 : 81.52%\n",
      "Epoch 41: training loss = 0.9798743499947784\n",
      "Training Accuracy after Epoch 41 : 81.68%\n",
      "Epoch 42: training loss = 0.9699957367047223\n",
      "Training Accuracy after Epoch 42 : 81.94%\n",
      "Epoch 43: training loss = 0.9606223233547585\n",
      "Training Accuracy after Epoch 43 : 82.04%\n",
      "Epoch 44: training loss = 0.9516820311015836\n",
      "Training Accuracy after Epoch 44 : 82.36%\n",
      "Epoch 45: training loss = 0.9426235943788845\n",
      "Training Accuracy after Epoch 45 : 82.46%\n",
      "Epoch 46: training loss = 0.9338323657712676\n",
      "Training Accuracy after Epoch 46 : 82.66%\n",
      "Epoch 47: training loss = 0.9253511653451765\n",
      "Training Accuracy after Epoch 47 : 82.79%\n",
      "Epoch 48: training loss = 0.9173292866053762\n",
      "Training Accuracy after Epoch 48 : 82.99%\n",
      "Epoch 49: training loss = 0.9092430004019078\n",
      "Training Accuracy after Epoch 49 : 83.16%\n",
      "Epoch 50: training loss = 0.9016295895505722\n",
      "Training Accuracy after Epoch 50 : 83.27%\n",
      "Epoch 51: training loss = 0.8937921881354851\n",
      "Training Accuracy after Epoch 51 : 83.36%\n",
      "Epoch 52: training loss = 0.8862636153302866\n",
      "Training Accuracy after Epoch 52 : 83.61%\n",
      "Epoch 53: training loss = 0.8786156676035807\n",
      "Training Accuracy after Epoch 53 : 83.74%\n",
      "Epoch 54: training loss = 0.8716650923079965\n",
      "Training Accuracy after Epoch 54 : 83.9%\n",
      "Epoch 55: training loss = 0.8644020841421404\n",
      "Training Accuracy after Epoch 55 : 84.0%\n",
      "Epoch 56: training loss = 0.8578382839893391\n",
      "Training Accuracy after Epoch 56 : 84.14%\n"
     ]
    }
   ],
   "source": [
    "W2,b2,W1,b1,li2,loss2=training(X_trainval,y_trainval,64,0.05,300,sigmoid, sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loss using configuration 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "loss= loss2\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Test Loss using configuration 2', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "#plt.scatter(epochs,loss,alpha=1,c='green')\n",
    "plt.plot(epochs,loss,c='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training accuracies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "accuracies= li2\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Training Accuracies using configuration 2', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(epochs,accuracies,alpha=1,c='green')\n",
    "plt.plot(epochs,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Validation data using configuration 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labely_test,labelpred=testing(W2,b2,W1,b1,X_testval,y_testval,sigmoid)\n",
    "accuracy_val=accuracy(labely_test, labelpred)\n",
    "print(\"Accuracy on Validation data using configuration 2: {}%\".format(accuracy_val))\n",
    "acc2=accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 3 :\n",
    "* Number of neurons = 128\n",
    "* Learning rate = 0.01\n",
    "* Iteratations =300\n",
    "* Activation Function : Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2,b2,W1,b1,li3,loss3=training(X_trainval,y_trainval,128,0.01,300,sigmoid,sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loss using configuration 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "loss= loss3\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Test Loss using configuration 3', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "#plt.scatter(epochs,loss,alpha=1,c='green')\n",
    "plt.plot(epochs,loss,c='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training accuracies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "accuracies= li3\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Training Accuracies using configuration 3', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(epochs,accuracies,alpha=1,c='green')\n",
    "plt.plot(epochs,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Validation data using configuration 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labely_test,labelpred=testing(W2,b2,W1,b1,X_testval,y_testval,sigmoid)\n",
    "accuracy_val=accuracy(labely_test, labelpred)\n",
    "print(\"Accuracy on Validation data using configuration 3: {}%\".format(accuracy_val))\n",
    "acc3=accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 4 :\n",
    "* Number of neurons = 128\n",
    "* Learning rate = 0.05\n",
    "* Iteratations =300\n",
    "* Activation Function : Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2,b2,W1,b1,li4,loss4=training(X_trainval,y_trainval,128,0.01,300,sigmoid,sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loss using configuration 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "loss= loss4\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Test Loss using configuration 4', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "#plt.scatter(epochs,loss,alpha=1,c='green')\n",
    "plt.plot(epochs,loss,c='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training accuracies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "accuracies= li4\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Training Accuracies using configuration 4', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(epochs,accuracies,alpha=1,c='green')\n",
    "plt.plot(epochs,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Validation data using configuration 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labely_test,labelpred=testing(W2,b2,W1,b1,X_testval,y_testval,sigmoid)\n",
    "accuracy_val=accuracy(labely_test, labelpred)\n",
    "print(\"Accuracy on Validation data using configuration 4: {}%\".format(accuracy_val))\n",
    "acc4=accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 5 :\n",
    "* Number of neurons = 64\n",
    "* Learning rate = 0.05\n",
    "* Iteratations =300\n",
    "* Activation Function : tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2,b2,W1,b1,li5,loss5=training(X_trainval,y_trainval,64,0.05,300,tanh, tanh_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loss using Configuration 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "loss= loss5\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Test Loss using configuration 5', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "#plt.scatter(epochs,loss,alpha=1,c='green')\n",
    "plt.plot(epochs,loss,c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Training Accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "accuracies= li5\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Training Accuracies using configuration 5', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(epochs,accuracies,alpha=1,c='green')\n",
    "plt.plot(epochs,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Validation Test data using Configuration 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labely_test,labelpred=testing(W2,b2,W1,b1,X_testval,y_testval,tanh)\n",
    "accuracy_val=accuracy(labely_test, labelpred)\n",
    "print(\"Accuracy on Validation data using configuration 5: {}%\".format(accuracy_val))\n",
    "acc5=accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the training accuracy observations as a whole :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Test Accuracies on Training Data', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.plot( epochs,li1, marker='o', markerfacecolor='blue', markersize=2, color='skyblue', linewidth=2, label=\"config 1\")\n",
    "plt.plot( epochs,li2, marker='', color='olive', linewidth=2,linestyle='dashed', label=\"config 2\")\n",
    "plt.plot(epochs,li3, marker='', color='blue', linewidth=2, label=\"config 3\")\n",
    "plt.plot(epochs,li4, marker='', color='red', linewidth=2, label=\"config 4\")\n",
    "plt.plot(epochs,li5, marker='', color='black', linewidth=2, label=\"config 5\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can see that when used configuration 5, the model gives highest accuracy of predictions. We can see a steep curve with slight disturbances, converging to an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the training loss observations as a whole :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Training Losses on Training Data', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Training Loss', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.plot( epochs,loss1, marker='o', markerfacecolor='blue', markersize=2, color='skyblue', linewidth=2, label=\"config 1\")\n",
    "plt.plot( epochs,loss2, marker='', color='olive', linewidth=2,linestyle='dashed', label=\"config 2\")\n",
    "plt.plot(epochs,loss3, marker='', color='blue', linewidth=2, label=\"config 3\")\n",
    "plt.plot(epochs,loss4, marker='', color='red', linewidth=2, label=\"config 4\")\n",
    "plt.plot(epochs,loss5, marker='', color='black', linewidth=2, label=\"config 5\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can see a significant decrease in training loss when used configuration 5, notice the steep curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Test accuracies with different learning rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=[0.01,0.05]\n",
    "accuracies=[acc1,acc2]\n",
    "plt.figure(figsize=(15,7))\n",
    "width=0.25\n",
    "plt.title('Trends in Test Accuracies with different learning rates', fontsize=15)\n",
    "plt.xlabel('Learning Rates', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(learning_rate,accuracies,alpha=1,c='green')\n",
    "plt.plot(learning_rate,accuracies,width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here when I increased the learning rate from 0.01 to 0.05 keeping number of neurons same i.e 64, I observed a noticeable increase in accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Test accuracies with different number of neurons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[64,128]\n",
    "accuracies=[acc1,acc3]\n",
    "plt.figure(figsize=(15,7))\n",
    "width=0.25\n",
    "plt.title('Trends in Test Accuracies with different number of neurons', fontsize=15)\n",
    "plt.xlabel('Neurons', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(neurons,accuracies,alpha=1,c='green')\n",
    "plt.plot(neurons,accuracies,width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here when I increased the number of neurons from 64 to 128 keeping learning rate = 0.01, I observed and slight increase in the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2,acc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Test accuracies using different activation functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation =['Sigmoid','Tanh']\n",
    "accuracies=[acc2,acc5]\n",
    "plt.figure(figsize=(15,7))\n",
    "width=0.25\n",
    "plt.title('Trends in Test Accuracies with different type of activations', fontsize=15)\n",
    "plt.xlabel('Activation', size=15)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(activation,accuracies,alpha=1,c='green')\n",
    "plt.plot(activation,accuracies,width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can see a noticeable increase in model performance when we used Tanh activation function.Tanh curve is steeper than the sigmoid curve, one of the reasons it helps the algorithm to converge is less number of epochs than sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot to see trends in accuracies over all configurations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = [1,2,3,4,5]\n",
    "accuracies= [acc1,acc2,acc3,acc4,acc5]\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Test Accuracies over configurations', fontsize=15)\n",
    "plt.xlabel('Configuration', size=15)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(configurations,accuracies,alpha=1,c='green')\n",
    "plt.plot(configurations,accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can see after taking into account all the results obtained using all possible configurations by tweaking Learning rates and number of  and type of non-linerity activation function in the hidden layer, we observed that our model performed very well when we used configurations 2 and 5. Hence we will be using the best configuration out of the two that is configuration 5 on our model to achieve good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Criteria :\n",
    "* For each configuration, the Testing function simply takes the updated weights retrieved from training function and runs our model on validation data thus returning probabilities of class labels. \n",
    "* Further, accuracy is calculated on validation data. I have tuned my weight parameters for 300 epochs and then tested my model since after 300th epoch, it was observed that the although the training accuracy was gradually increasing, the model showed almost no improvement in performance and no decrease in loss, that is, it has reached the point of convergence. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 :\n",
    "* After evaluating our model on Validation dataset using 4 configurations we will be training our model using the best configuration that is configuration 5 on our entire original Training dataset of 60,000 samples. Furthermore, we will evaluate our model on original Test data consisting of 10,000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Configuration Used :\n",
    "* Number of neurons = 64\n",
    "* Learning rate = 0.05\n",
    "* Iteratations =300\n",
    "* Activation Function : tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model using Configuration 5 on Entire Training Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2,b2,W1,b1,accu_train,losstrain=training(X_train,y_train,64,0.05,300,tanh,tanh_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "loss= losstrain\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Test Loss using configuration 2', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "#plt.scatter(epochs,loss,alpha=1,c='green')\n",
    "plt.plot(epochs,loss,c='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can see that the loss is decreasing gradually, that is our model is learning by taking into consideration the loss at after each iteration, and then computing the change in loss w.r.t weights. This gradient information is computed and back propogated to the hidden layers and input layer. Using this information, weights are updated again and these weights are used in feed forward network for the next iteration. In this way our model learns from the loss obtained after each epoch so that it performs better in the next epoch thus significantly reducing the loss and increasing performance of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Training Accuracies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,300+1))\n",
    "accuracies= accu_train\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Trends in Test Accuracies on Entire Training Data', fontsize=15)\n",
    "plt.xlabel('Epochs', size=14)\n",
    "plt.ylabel('Accuracies', fontsize=14)\n",
    "plt.tick_params(labelsize=12);\n",
    "plt.scatter(epochs,accuracies,alpha=1,c='green')\n",
    "plt.plot(epochs,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can see a significant increase in our training accuracies using tanh activation function.Tanh curve is steeper than the sigmoid curve, one of the reasons it helps the algorithm to converge is less number of epochs than sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Entire Test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labely_test,labelpred=testing(W2,b2,W1,b1,X_test,y_test,tanh)\n",
    "accuracy_val=accuracy(labely_test, labelpred)\n",
    "print(\"Accuracy on Entire Testing data using configuration 5: {}%\".format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see here that our model has performed significantly well on our Test data with a 91.3% accuracy. As our algorithm steps in further iterations, it's loss gradually decreases compared to the initial iterations where we can see a noticeable increase in accuracy and decrease in loss. With a 0.05 learning rate, slowly our algorithm tries to converge to a better solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here I have implemented a Multiclass Neural Network classifier with one hidden layer. I have used idx2numpy library to convert the input file into a numpy array for conveneince. I reshaped the original training and testing data of dimensions (60,000 * 28 * 28) into 2D numpy array 60000 * 784. I have split the original Training data in ratio 70: 30 into training and validation data for Q3. \n",
    "* Each of the inputs will be connected with each of the following layer hidden neurons and will have an associated weight with it. Further each of the neurons in the hidden layer will be connected to each of the output layer neurons/class labels. Hence, I have initialized random weights and first set the bias to zero.\n",
    "* In the training function, our model will learn based on our training data. First, weights and bias will be initialized. Then in Feed Forward function we simply take the weights and bias and each of our input instances and compute the dot product of each feature vector in input data with all the weight vectors and add bias. \n",
    "* This outcome is passed through a non-linearity activation function. Here I have used two functions, Sigmoid and Tanh. This intermediate output is then carried forward in our network.\n",
    "* This intermediate output acting as input for the next layer, i.e the output layer. Dot product of this intermediate output with new weight vectors will be computed along with adding bias. The final outcome at the output layer will be then passed through another non-linearity activation function called as Softmax function which squeezes all the outcomes between 0 and 1 such that they all add up to 1. Hence what we receive is 10 probabilites for each instance which implies the probabilites that the input instance belongs to the class labels.\n",
    "* Loss is computed at each epoch using Cross entropy loss since it is used to deal with computing losses of outcomes that are in the form of probabilites. Backpropogation function calculates loss i.e error of each predicted output, and then computes the change in loss with respect to weights at each layer(gradient). This gradient information is sent back to the hidden layers so that the weights can get updated using Update Weights function for the next run. \n",
    "* Accuracy for each epoch is calculated by comparing argmax of each output vector (which is our corresponding class label) and the true class labels. \n",
    "* In such way, our model keeps learning from the loss gradually, and we notice an increase in accuracy for training data.\n",
    "* Using each of the mentioned configurations ,we evaluate our model on validation data to check which configuration gives a better performance of our model. \n",
    "* Hence I used the best configuration, 5th configuration and using this one, I trained my model using the entire training data 60000 instances and then evaluated it on my test data having 10,000 images and noted accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    " \n",
    "# PERCEPTRON  <br>\n",
    "</div>\n",
    "<div style=\"text-align: justify\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here my baseline model is my previous implementation of Single layer Multiclass Perceptron to compare its performance with current Neural Network Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to prepend 1 to input data for bias term :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones(X): \n",
    "    return np.hstack((np.ones((X.shape[0], 1),dtype=int), X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=add_ones(X_train)\n",
    "X_test=add_ones(X_test)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def training(X_train, y_train):\n",
    "        \n",
    "        dotprod=np.ndarray(10,dtype=np.float32)\n",
    "        weights=np.zeros((10,X_train.shape[1]), dtype=np.float32)\n",
    "        iterations=300                           # Number of epochs\n",
    "        alpha=0.05                              # Learning Rate\n",
    "        accu=[]\n",
    "        co=[]\n",
    "        for iter in range(iterations) : \n",
    "            count=0\n",
    "            for i in range(X_train.shape[0]): # iterating through each feature vector\n",
    "                for j in range(10):     # weight vectors equivalent to 10 class labels \n",
    "                    dotprod[j] = np.dot(X_train[i],weights[j])\n",
    "        \n",
    "                ind=y_train[i]                 # true class label\n",
    "                maxi=np.argmax(dotprod,axis=None)\n",
    "                y_pred=maxi             # got predicted corresponding class label\n",
    "       \n",
    "                if(y_pred!=y_train[i]):\n",
    "                #subtracting x input from old weight vector coress to predicted label \n",
    "                    weights[maxi] -= alpha*X_train[i]\n",
    "                  \n",
    "                #adding x input to old weight vector coress to true label \n",
    "                    weights[ind] += alpha*X_train[i]\n",
    "                    \n",
    "                elif(y_pred==y_train[i]):\n",
    "                    count=count+1\n",
    "                    \n",
    "            accuracy = round(((count/y_train.shape[0])*100), 2)\n",
    "            print(\"Accuracy after epoch \"+ str(iter)  + \" is : \",accuracy)\n",
    "            accu.append(accuracy)\n",
    "            #print(\"Count of correctly classified samples is : \" , count)\n",
    "            co.append(count)\n",
    "            \n",
    "        #zippedList =  list(zip(accu,co))\n",
    "        #df=pd.DataFrame(zippedList, columns = ['Accuracy in %', 'Correctly_classified_samples'],index=None)\n",
    "        #print(df)\n",
    " \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def testing(X_test,w):\n",
    "        \n",
    "        ypred=[]\n",
    "        activ=[]\n",
    "        activ=np.dot(X_test,w.T)     #calculating activation\n",
    "        \n",
    "        for i in range(activ.shape[0]):\n",
    "            ypred.append(np.argmax(activ[i]))\n",
    "\n",
    "        return ypred           #predicted class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=training(X_train, y_train)  #got updated weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation for Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_for_Training=testing(X_train,w)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(y_train.shape[0]):\n",
    "    if predictions_for_Training[i]==y_train[i]:\n",
    "        count=count+1\n",
    "print(\"Number of samples correctly classified for training data : \", count)\n",
    "print('Accuracy in % :', accuracy_score(y_train,predictions_for_Training)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation for Testing Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_for_Testing=testing(X_test,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(y_test.shape[0]):\n",
    "    if predictions_for_Testing[i]==y_test[i]:\n",
    "        count=count+1\n",
    "\n",
    "print(\"Number of samples correctly classified for testing data : \", count)\n",
    "print('Accuracy in % :', accuracy_score(y_test,predictions_for_Testing)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here I have implemented a single layer, Multi-class perceptron. I have prepended a column of 1's to the input data for simplification in case of bias term. A numpy weight array of dimensions(10,785) where (785 since 784 dimensions + bias term) has been declared.\n",
    "\n",
    "* In the training function, our model will learn based on our training data. With learning rate of 0.01 at each epoch, it calculates the dot product of each feature vector in input data with all the 10 weight vectors coressponding to the given 10 class labels. The index of the maximum dot product-value will be our coressponding  predicted class label.\n",
    "\n",
    "\n",
    "* If our predictions are correct, then simply store the count of correct prediction at each iteration. But if our prediction is incorrect, then the weights for the correct class are increased by x and the weights for the incorrectly predicted class are decreased by x. \n",
    "\n",
    "* Keep updating the weights for each iterations/epoch and compute the training accuracy to see variations. We will see that the accuracy has increased gradually. \n",
    "\n",
    "* But after a certain epoch, we may see that accuracy tends to be almost 100% that is tends to overfit our model on training data and perform poorly on testing data.\n",
    "\n",
    "* Testing function simply takes these updated weights retrieved from training function and runs our model on test data thus returning predicted class labels. \n",
    "* Further, accuracy is calculated for test data. I have tuned my weight parameters for 800 epochs and then tested my model since after th epoch, it was observed that the although the training accuracy was substantially increasing, the testing accuracy began to decline noticably, that is, it was trying to overfit the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Comparison :\n",
    "* As we can see here that when I tested my data on both the models, Neural Network Classifier and Multiclass Single layer Perceptron, we could see thart Neural Network Classifier performed very well compared to the Multiclass Perceptron. \n",
    "* By evaluating our Neural Network Classifier on Test data, we achieved the accuracy of 91.3% compared to when we tested our Multiclass Perceptron where the test accuracy was less, 87.16%. Here, in the latter model, there are no hidden layers involved. * Single Layer Perceptrons are used for classfying linearly separable data. The dot product of input values with weights are computed and weights are updated in a linear fashion. Whereas Neural Network Classifiers can be used to classify linearly inseparable data using Non-linearity Activation Functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li1,li2,li3,li4,loss5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1,acc2,acc3,acc4,acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
