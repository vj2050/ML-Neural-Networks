{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the file into numpy array of dimensions (60000,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertfunc(file):\n",
    "    arr = idx2numpy.convert_from_file(file)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=convertfunc('train-images.idx3-ubyte')\n",
    "y_train=convertfunc('train-labels.idx1-ubyte')\n",
    "X_test=convertfunc('t10k-images.idx3-ubyte')\n",
    "y_test=convertfunc('t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapefunc(array,rows,columns):\n",
    "    array=array.reshape(rows,columns)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=reshapefunc(X_train,60000,784)\n",
    "X_test=reshapefunc(X_test,10000,784)\n",
    "#y_train=reshapefunc(y_train,1,60000)\n",
    "#y_test=reshapefunc(y_test,1,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape, y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One hot label encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train)\n",
    "y_train=np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(inputs, hNeurons, oNeurons):\n",
    "\n",
    "    weights1=np.random.randn(inputs,hNeurons)*np.sqrt(1./inputs)    #784 rows 5 columns\n",
    "    bias1=np.zeros((1, hNeurons))*np.sqrt(1./inputs)\n",
    "    #hence x.w = 1,5 dimensions + bias [1,5]\n",
    "    weights2=np.random.randn(hNeurons,oNeurons)*np.sqrt(1./hNeurons)\n",
    "    #hence x.w= 5,10 dimensions + bias [1,10]\n",
    "    bias2= np.zeros((1, oNeurons))*np.sqrt(1./hNeurons)\n",
    "\n",
    "    #ouptut vecotr is going to be of dimensions [1,10] then softmax\n",
    "    #W1.shape , b1.shape, W2.shape, b2.shape\n",
    "    \n",
    "    return weights1, bias1, weights2, bias2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    act = 1. / (1. + np.exp(-z))\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    softact = np.exp(z)/np.sum(np.exp(z), axis=0)                                       #- np.max(z, axis=1, keepdims=True)\n",
    "    return softact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(X_train,W1,b1,W2,b2):\n",
    "    z1= np.dot(X_train,W1)+b1\n",
    "    h1=sigmoid(z1)\n",
    "    \n",
    "    z2=np.dot(h1,W2)+b2\n",
    "    output=softmax(z2)\n",
    "    return z1, h1, z2, output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dotprod1, h1, dotprod2, output=feedforward(X_train[:30000],W1,b1,W2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(ypred,y_train):     \n",
    "    n_samples = y_train.shape[0]\n",
    "    L = (ypred-y_train)/n_samples\n",
    "    return L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef crossentropy(ypred, y_train):       #loss function\\n    n_samples = y_train.shape[0]\\n    logp = - np.log(ypred[np.arange(n_samples), y_train.argmax(axis=1)])\\n    loss = np.sum(logp)/n_samples\\n    return loss\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def crossentropy(ypred, y_train):       #loss function\n",
    "    n_samples = y_train.shape[0]\n",
    "    logp = - np.log(ypred[np.arange(n_samples), y_train.argmax(axis=1)])\n",
    "    loss = np.sum(logp)/n_samples\n",
    "    return loss\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(ypred,y_train):\n",
    "    L_sum = np.sum(np.multiply(y_train, np.log(ypred)))\n",
    "    m = y_train.shape[0]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(h1):\n",
    "    deriv=h1*(1-h1)\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(h1,W2,ypred,y_train):\n",
    "    \n",
    "    #Calculate error\n",
    "    L=error(ypred,y_train)\n",
    "    \n",
    "    # z1, h1, z2, output\n",
    "    # z1, and z2 => dot products\n",
    "    # h1 and output => activations\n",
    "    change_output=L\n",
    "    #dL/dW2\n",
    "    delta_W2= np.dot(h1.T,change_output)\n",
    "    \n",
    "    #dL/db2\n",
    "    delta_b2= np.sum(change_output,axis=0)\n",
    "    \n",
    "    #dL/dh1\n",
    "    delta_h1=np.dot(change_output,W2.T)\n",
    "    \n",
    "    change_h1= delta_h1*sigmoid_derivative(h1)\n",
    "    \n",
    "    #dL/dW1\n",
    "    delta_W1= np.dot(X_train[:30000].T, change_h1)\n",
    "    \n",
    "    #dL/db1\n",
    "    delta_b1= np.sum(change_h1,axis=0)\n",
    "    \n",
    "    return delta_W2, delta_b2, delta_W1, delta_b1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta_W2, delta_b2, delta_W1, delta_b1= backpropagation(output,y_train[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(alpha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1):\n",
    "    W2= W2- alpha*(delta_W2)\n",
    "    b2= b2 - alpha*(delta_b2)\n",
    "    W1= W1- alpha*(delta_W1)\n",
    "    b1=b1- alpha*(delta_b1)\n",
    "    \n",
    "    return W2,b2,W1,b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W2,b2,W1,b1=update_weights(0.001,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    hNeurons=64                               #hidden neuron\n",
    "    inputs=X_train.shape[1]                   #number of inputs\n",
    "    oNeurons=y_train.shape[1]\n",
    "    aplha= 0.005                         #learning rate\n",
    "    epochs=100\n",
    "    \n",
    "    \n",
    "    #initialize weights\n",
    "    W1, b1, W2, b2= initialize_weights(inputs,hNeurons,oNeurons)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #feed forward\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_train[:30000],W1,b1,W2,b2)\n",
    "\n",
    "        #back propogate\n",
    "        delta_W2, delta_b2, delta_W1, delta_b1= backpropagation(h1,W2,output,y_train[:30000])\n",
    "        #update weights using gradient descent\n",
    "        W2,b2,W1,b1=update_weights(aplha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1)\n",
    "\n",
    "        #again feed forward using updated weights\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_train[:30000],W1,b1,W2,b2)\n",
    "\n",
    "        #Calculating Total Loss\n",
    "        loss=crossentropy(output,y_train[:30000])                 # put in training func later\n",
    "        print(\"loss on total training set is : \", loss)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on total training set is :  10.396443529951062\n",
      "loss on total training set is :  10.3808189647438\n",
      "loss on total training set is :  10.365155749450599\n",
      "loss on total training set is :  10.349403219409451\n",
      "loss on total training set is :  10.333766840672817\n",
      "loss on total training set is :  10.319180312619235\n",
      "loss on total training set is :  10.305232184664327\n",
      "loss on total training set is :  10.291283199396991\n",
      "loss on total training set is :  10.277463560862525\n",
      "loss on total training set is :  10.26384937418235\n",
      "loss on total training set is :  10.250521241800557\n",
      "loss on total training set is :  10.237333436657034\n",
      "loss on total training set is :  10.22456004803703\n",
      "loss on total training set is :  10.212406374562214\n",
      "loss on total training set is :  10.200357802459838\n",
      "loss on total training set is :  10.188576448666456\n",
      "loss on total training set is :  10.177393640220728\n",
      "loss on total training set is :  10.166618795097726\n",
      "loss on total training set is :  10.15609761738266\n",
      "loss on total training set is :  10.145912229112989\n",
      "loss on total training set is :  10.135899822251481\n",
      "loss on total training set is :  10.126100807140688\n",
      "loss on total training set is :  10.116641588038277\n",
      "loss on total training set is :  10.107487249101052\n",
      "loss on total training set is :  10.09803038122889\n",
      "loss on total training set is :  10.089008176181963\n",
      "loss on total training set is :  10.0803138901674\n",
      "loss on total training set is :  10.072045907064112\n",
      "loss on total training set is :  10.064092546938666\n",
      "loss on total training set is :  10.056385645695158\n",
      "loss on total training set is :  10.048717849987948\n",
      "loss on total training set is :  10.041490555239829\n",
      "loss on total training set is :  10.034292232845676\n",
      "loss on total training set is :  10.027394441004782\n",
      "loss on total training set is :  10.020894040942762\n",
      "loss on total training set is :  10.014570811552474\n",
      "loss on total training set is :  10.00810524680131\n",
      "loss on total training set is :  10.001599014058193\n",
      "loss on total training set is :  9.995154369563007\n",
      "loss on total training set is :  9.988701328969602\n",
      "loss on total training set is :  9.982607446873336\n",
      "loss on total training set is :  9.976915684944858\n",
      "loss on total training set is :  9.971382998938731\n",
      "loss on total training set is :  9.965775828798435\n",
      "loss on total training set is :  9.960014565747622\n",
      "loss on total training set is :  9.954292475419782\n",
      "loss on total training set is :  9.948738166147125\n",
      "loss on total training set is :  9.9431419745066\n",
      "loss on total training set is :  9.937484972613339\n",
      "loss on total training set is :  9.931882746226611\n",
      "loss on total training set is :  9.926459477016916\n",
      "loss on total training set is :  9.921262793226543\n",
      "loss on total training set is :  9.916205323110725\n",
      "loss on total training set is :  9.911260297140148\n",
      "loss on total training set is :  9.906402415613961\n",
      "loss on total training set is :  9.901503833195356\n",
      "loss on total training set is :  9.896664693805333\n",
      "loss on total training set is :  9.891960282459518\n",
      "loss on total training set is :  9.887405772956164\n",
      "loss on total training set is :  9.882947241275831\n",
      "loss on total training set is :  9.878447200497359\n",
      "loss on total training set is :  9.873918050558416\n",
      "loss on total training set is :  9.869440639226962\n",
      "loss on total training set is :  9.864961766333986\n",
      "loss on total training set is :  9.86044573104518\n",
      "loss on total training set is :  9.855954652693239\n",
      "loss on total training set is :  9.851501985857888\n",
      "loss on total training set is :  9.847064756471736\n",
      "loss on total training set is :  9.842612401260045\n",
      "loss on total training set is :  9.838118519611344\n",
      "loss on total training set is :  9.833776275823826\n",
      "loss on total training set is :  9.829554216297323\n",
      "loss on total training set is :  9.825375893319988\n",
      "loss on total training set is :  9.821252258174251\n",
      "loss on total training set is :  9.817081230462646\n",
      "loss on total training set is :  9.812895417406434\n",
      "loss on total training set is :  9.808848402188818\n",
      "loss on total training set is :  9.804964117979047\n",
      "loss on total training set is :  9.801204459218674\n",
      "loss on total training set is :  9.7974108718096\n",
      "loss on total training set is :  9.793523489279385\n",
      "loss on total training set is :  9.789574552781774\n",
      "loss on total training set is :  9.785691573962938\n",
      "loss on total training set is :  9.781897541277777\n",
      "loss on total training set is :  9.778196999564289\n",
      "loss on total training set is :  9.774628131053573\n",
      "loss on total training set is :  9.771114841787574\n",
      "loss on total training set is :  9.76758482929096\n",
      "loss on total training set is :  9.76401002335486\n",
      "loss on total training set is :  9.760434201107252\n",
      "loss on total training set is :  9.756917600937951\n",
      "loss on total training set is :  9.75347647710903\n",
      "loss on total training set is :  9.750073880163463\n",
      "loss on total training set is :  9.746753701779198\n",
      "loss on total training set is :  9.743552409767517\n",
      "loss on total training set is :  9.740411258192246\n",
      "loss on total training set is :  9.737319300728041\n",
      "loss on total training set is :  9.73426896395498\n",
      "loss on total training set is :  9.731239001379812\n",
      "loss on total training set is :  9.728226586001838\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
