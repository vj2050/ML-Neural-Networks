{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAISHNAVI JAMDADE(TM39453)\n",
    "### HOMEWORK 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the file into numpy array of dimensions (60000,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertfunc(file):\n",
    "    arr = idx2numpy.convert_from_file(file)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=convertfunc('train-images.idx3-ubyte')\n",
    "y_train=convertfunc('train-labels.idx1-ubyte')\n",
    "X_test=convertfunc('t10k-images.idx3-ubyte')\n",
    "y_test=convertfunc('t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapefunc(array,rows,columns):\n",
    "    array=array.reshape(rows,columns)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=reshapefunc(X_train,60000,784)\n",
    "X_test=reshapefunc(X_test,10000,784)\n",
    "#y_train=reshapefunc(y_train,1,60000)\n",
    "#y_test=reshapefunc(y_test,1,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape, y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One hot label encoding for output labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(inputs, hNeurons, oNeurons):\n",
    "\n",
    "    weights1=np.random.randn(inputs,hNeurons)*np.sqrt(1./inputs)    #784 rows 5 columns\n",
    "    bias1=np.zeros((1, hNeurons))*np.sqrt(1./inputs)\n",
    "    #hence x.w = 1,5 dimensions + bias [1,5]\n",
    "    weights2=np.random.randn(hNeurons,oNeurons)*np.sqrt(1./hNeurons)\n",
    "    #hence x.w= 5,10 dimensions + bias [1,10]\n",
    "    bias2= np.zeros((1, oNeurons))*np.sqrt(1./hNeurons)\n",
    "\n",
    "    #ouptut vecotr is going to be of dimensions [1,10] then softmax\n",
    "    #W1.shape , b1.shape, W2.shape, b2.shape\n",
    "    \n",
    "    return weights1, bias1, weights2, bias2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    act = 1. / (1. + np.exp(-z))\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef softmax(z):\\n    softact = np.exp(z - np.max(z, axis=1, keepdims=True))\\n    softact=softact/np.sum(softact, axis=1, keepdims=True)\\n    \\n    return softact\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalising the values:\n",
    "\n",
    "def softmax(z):\n",
    "    softact = np.exp(z)/np.sum(np.exp(z), axis=1,keepdims=True)                                       \n",
    "    return softact\n",
    "\n",
    "'''\n",
    "def softmax(z):\n",
    "    softact = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    softact=softact/np.sum(softact, axis=1, keepdims=True)\n",
    "    \n",
    "    return softact\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(X_train,W1,b1,W2,b2):\n",
    "    z1= np.dot(X_train,W1)+b1\n",
    "    h1=sigmoid(z1)\n",
    "    \n",
    "    z2=np.dot(h1,W2)+b2\n",
    "    output=softmax(z2)\n",
    "    return z1, h1, z2, output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(ypred,y_train):     \n",
    "    n_samples = y_train.shape[0]\n",
    "    L = (ypred-y_train)/n_samples\n",
    "    return L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crossentropy(ypred,y_train):\n",
    "    L_sum = np.sum(np.multiply(y_train, np.log(ypred)))\n",
    "    num_samples = y_train.shape[0]\n",
    "    L = -(1./num_samples) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate Sigmoid Derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(h1):\n",
    "    deriv=h1*(1-h1)\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Function to compute Gradient Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(h1,W2,ypred,y_train):\n",
    "    \n",
    "    #Calculate error\n",
    "    L=error(ypred,y_train)\n",
    "    \n",
    "    # z1, h1, z2, output\n",
    "    # z1, and z2 => dot products\n",
    "    # h1 and output => activations\n",
    "    change_output=L\n",
    "    #dL/dW2\n",
    "    delta_W2= np.dot(h1.T,change_output)\n",
    "    \n",
    "    #dL/db2\n",
    "    delta_b2= np.sum(change_output,axis=0,keepdims=True)\n",
    "    \n",
    "    #dL/dh1\n",
    "    delta_h1=np.dot(change_output,W2.T)\n",
    "\n",
    "    change_h1= delta_h1*sigmoid_derivative(h1)\n",
    "    \n",
    "    #dL/dW1\n",
    "    delta_W1= np.dot(X_train.T, change_h1)\n",
    "    \n",
    "    #dL/db1\n",
    "    delta_b1= np.sum(change_h1,axis=0,keepdims=True)\n",
    "    \n",
    "    return delta_W2, delta_b2, delta_W1, delta_b1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(alpha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1):\n",
    "    W2= W2- alpha*(delta_W2)\n",
    "    b2= b2 - alpha*(delta_b2)\n",
    "    W1= W1- alpha*(delta_W1)\n",
    "    b1=b1- alpha*(delta_b1)\n",
    "    \n",
    "    return W2,b2,W1,b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    hNeurons=128                               #hidden neuron\n",
    "    inputs=X_train.shape[1]                   #number of inputs\n",
    "    oNeurons=y_train.shape[1]\n",
    "    alpha= 0.05                        #learning rate\n",
    "    epochs=300\n",
    "    \n",
    "    #retriving the actual labels for y_train:\n",
    "    labely_train=np.argmax(y_train,axis=1)\n",
    "    \n",
    "    #initialize weights\n",
    "    W1, b1, W2, b2= initialize_weights(inputs,hNeurons,oNeurons)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #feed forward\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_train,W1,b1,W2,b2)\n",
    "\n",
    "        #back propogate\n",
    "        delta_W2, delta_b2, delta_W1, delta_b1= backpropagation(h1,W2,output,y_train)\n",
    "        \n",
    "        #update weights using gradient descent\n",
    "        W2,b2,W1,b1=update_weights(alpha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1)\n",
    "\n",
    "        #again feed forward using updated weights\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_train,W1,b1,W2,b2)\n",
    "\n",
    "        #Calculating Total Loss\n",
    "        loss=crossentropy(output,y_train)                 # put in training func later\n",
    "        print(\"Epoch {}: training loss = {}\".format(i + 1,loss))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Retrieving the corresponding class labels:\n",
    "        labelpred=np.argmax(output,axis=1)\n",
    "        \n",
    "        #Training Accuracy:\n",
    "        accuracy = round((accuracy_score(labely_train, labelpred)*100),2)\n",
    "        print(\"Training Accuracy after Epoch {} : {}%\".format(i+1, accuracy))\n",
    "        \n",
    "    return W2,b2,W1,b1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.3513398081769155\n",
      "Training Accuracy after Epoch 1 : 14.49%\n",
      "Epoch 2: training loss = 2.189481571457147\n",
      "Training Accuracy after Epoch 2 : 22.18%\n",
      "Epoch 3: training loss = 2.069003881318173\n",
      "Training Accuracy after Epoch 3 : 30.77%\n",
      "Epoch 4: training loss = 1.9683907597418244\n",
      "Training Accuracy after Epoch 4 : 37.63%\n",
      "Epoch 5: training loss = 1.8813677534692541\n",
      "Training Accuracy after Epoch 5 : 43.22%\n",
      "Epoch 6: training loss = 1.8051914989300661\n",
      "Training Accuracy after Epoch 6 : 47.92%\n",
      "Epoch 7: training loss = 1.736564534157259\n",
      "Training Accuracy after Epoch 7 : 52.12%\n",
      "Epoch 8: training loss = 1.6734071611577965\n",
      "Training Accuracy after Epoch 8 : 55.7%\n",
      "Epoch 9: training loss = 1.6156318852939355\n",
      "Training Accuracy after Epoch 9 : 59.05%\n",
      "Epoch 10: training loss = 1.5632779428533272\n",
      "Training Accuracy after Epoch 10 : 62.08%\n",
      "Epoch 11: training loss = 1.5144180523953739\n",
      "Training Accuracy after Epoch 11 : 64.78%\n",
      "Epoch 12: training loss = 1.4693556533124985\n",
      "Training Accuracy after Epoch 12 : 66.97%\n",
      "Epoch 13: training loss = 1.4279957315212672\n",
      "Training Accuracy after Epoch 13 : 68.78%\n",
      "Epoch 14: training loss = 1.389127811111504\n",
      "Training Accuracy after Epoch 14 : 70.38%\n",
      "Epoch 15: training loss = 1.3536316310177654\n",
      "Training Accuracy after Epoch 15 : 71.75%\n",
      "Epoch 16: training loss = 1.3199354654874105\n",
      "Training Accuracy after Epoch 16 : 72.91%\n",
      "Epoch 17: training loss = 1.2891768244747985\n",
      "Training Accuracy after Epoch 17 : 73.9%\n",
      "Epoch 18: training loss = 1.2607289155639376\n",
      "Training Accuracy after Epoch 18 : 74.82%\n",
      "Epoch 19: training loss = 1.2334942221448435\n",
      "Training Accuracy after Epoch 19 : 75.59%\n",
      "Epoch 20: training loss = 1.207833939081992\n",
      "Training Accuracy after Epoch 20 : 76.27%\n",
      "Epoch 21: training loss = 1.183394721449625\n",
      "Training Accuracy after Epoch 21 : 76.96%\n",
      "Epoch 22: training loss = 1.1601683821999471\n",
      "Training Accuracy after Epoch 22 : 77.6%\n",
      "Epoch 23: training loss = 1.1379807641675908\n",
      "Training Accuracy after Epoch 23 : 78.14%\n",
      "Epoch 24: training loss = 1.1169598696430156\n",
      "Training Accuracy after Epoch 24 : 78.66%\n",
      "Epoch 25: training loss = 1.0968684648394929\n",
      "Training Accuracy after Epoch 25 : 79.12%\n",
      "Epoch 26: training loss = 1.0776879948710982\n",
      "Training Accuracy after Epoch 26 : 79.58%\n",
      "Epoch 27: training loss = 1.0592774115455226\n",
      "Training Accuracy after Epoch 27 : 79.98%\n",
      "Epoch 28: training loss = 1.0415335622013453\n",
      "Training Accuracy after Epoch 28 : 80.32%\n",
      "Epoch 29: training loss = 1.0246228927506182\n",
      "Training Accuracy after Epoch 29 : 80.69%\n",
      "Epoch 30: training loss = 1.0086055950593704\n",
      "Training Accuracy after Epoch 30 : 80.99%\n",
      "Epoch 31: training loss = 0.9931866443391872\n",
      "Training Accuracy after Epoch 31 : 81.25%\n",
      "Epoch 32: training loss = 0.978194160400195\n",
      "Training Accuracy after Epoch 32 : 81.53%\n",
      "Epoch 33: training loss = 0.9635895045329098\n",
      "Training Accuracy after Epoch 33 : 81.79%\n",
      "Epoch 34: training loss = 0.949401022084481\n",
      "Training Accuracy after Epoch 34 : 82.07%\n",
      "Epoch 35: training loss = 0.9357883518230441\n",
      "Training Accuracy after Epoch 35 : 82.33%\n",
      "Epoch 36: training loss = 0.9226660697868295\n",
      "Training Accuracy after Epoch 36 : 82.52%\n",
      "Epoch 37: training loss = 0.90991745566256\n",
      "Training Accuracy after Epoch 37 : 82.74%\n",
      "Epoch 38: training loss = 0.8976106784271337\n",
      "Training Accuracy after Epoch 38 : 82.97%\n",
      "Epoch 39: training loss = 0.8858521139191287\n",
      "Training Accuracy after Epoch 39 : 83.23%\n",
      "Epoch 40: training loss = 0.8744440069722683\n",
      "Training Accuracy after Epoch 40 : 83.43%\n",
      "Epoch 41: training loss = 0.8634616661832424\n",
      "Training Accuracy after Epoch 41 : 83.58%\n",
      "Epoch 42: training loss = 0.852856966854795\n",
      "Training Accuracy after Epoch 42 : 83.76%\n",
      "Epoch 43: training loss = 0.8426419328202966\n",
      "Training Accuracy after Epoch 43 : 83.91%\n",
      "Epoch 44: training loss = 0.8327837519889951\n",
      "Training Accuracy after Epoch 44 : 84.06%\n",
      "Epoch 45: training loss = 0.8232018924288107\n",
      "Training Accuracy after Epoch 45 : 84.24%\n",
      "Epoch 46: training loss = 0.8140038629889611\n",
      "Training Accuracy after Epoch 46 : 84.35%\n",
      "Epoch 47: training loss = 0.804995748676507\n",
      "Training Accuracy after Epoch 47 : 84.52%\n",
      "Epoch 48: training loss = 0.7962287963113189\n",
      "Training Accuracy after Epoch 48 : 84.68%\n",
      "Epoch 49: training loss = 0.7878076533959923\n",
      "Training Accuracy after Epoch 49 : 84.81%\n",
      "Epoch 50: training loss = 0.7797084287363326\n",
      "Training Accuracy after Epoch 50 : 84.96%\n",
      "Epoch 51: training loss = 0.7717945570133664\n",
      "Training Accuracy after Epoch 51 : 85.04%\n",
      "Epoch 52: training loss = 0.7641412836653444\n",
      "Training Accuracy after Epoch 52 : 85.22%\n",
      "Epoch 53: training loss = 0.7567121429284828\n",
      "Training Accuracy after Epoch 53 : 85.27%\n",
      "Epoch 54: training loss = 0.7495216850590283\n",
      "Training Accuracy after Epoch 54 : 85.45%\n",
      "Epoch 55: training loss = 0.7423987728799644\n",
      "Training Accuracy after Epoch 55 : 85.53%\n",
      "Epoch 56: training loss = 0.7355072658126581\n",
      "Training Accuracy after Epoch 56 : 85.69%\n",
      "Epoch 57: training loss = 0.7288568726241427\n",
      "Training Accuracy after Epoch 57 : 85.73%\n",
      "Epoch 58: training loss = 0.7223741742373148\n",
      "Training Accuracy after Epoch 58 : 85.85%\n",
      "Epoch 59: training loss = 0.7161022177381519\n",
      "Training Accuracy after Epoch 59 : 85.92%\n",
      "Epoch 60: training loss = 0.7099990648385224\n",
      "Training Accuracy after Epoch 60 : 86.04%\n",
      "Epoch 61: training loss = 0.7040871624095092\n",
      "Training Accuracy after Epoch 61 : 86.1%\n",
      "Epoch 62: training loss = 0.698308114220222\n",
      "Training Accuracy after Epoch 62 : 86.22%\n",
      "Epoch 63: training loss = 0.6927128417135182\n",
      "Training Accuracy after Epoch 63 : 86.29%\n",
      "Epoch 64: training loss = 0.687281649815028\n",
      "Training Accuracy after Epoch 64 : 86.39%\n",
      "Epoch 65: training loss = 0.6820131256413811\n",
      "Training Accuracy after Epoch 65 : 86.46%\n",
      "Epoch 66: training loss = 0.6768144895643653\n",
      "Training Accuracy after Epoch 66 : 86.53%\n",
      "Epoch 67: training loss = 0.6717276624859655\n",
      "Training Accuracy after Epoch 67 : 86.61%\n",
      "Epoch 68: training loss = 0.6666846723657571\n",
      "Training Accuracy after Epoch 68 : 86.69%\n",
      "Epoch 69: training loss = 0.6617855206233565\n",
      "Training Accuracy after Epoch 69 : 86.74%\n",
      "Epoch 70: training loss = 0.656989379182392\n",
      "Training Accuracy after Epoch 70 : 86.84%\n",
      "Epoch 71: training loss = 0.6523336032731529\n",
      "Training Accuracy after Epoch 71 : 86.86%\n",
      "Epoch 72: training loss = 0.6477260562887339\n",
      "Training Accuracy after Epoch 72 : 86.94%\n",
      "Epoch 73: training loss = 0.6433311445290015\n",
      "Training Accuracy after Epoch 73 : 86.98%\n",
      "Epoch 74: training loss = 0.6389481283090125\n",
      "Training Accuracy after Epoch 74 : 87.11%\n",
      "Epoch 75: training loss = 0.6347403975219319\n",
      "Training Accuracy after Epoch 75 : 87.13%\n",
      "Epoch 76: training loss = 0.6305231161095157\n",
      "Training Accuracy after Epoch 76 : 87.23%\n",
      "Epoch 77: training loss = 0.6264012709030524\n",
      "Training Accuracy after Epoch 77 : 87.27%\n",
      "Epoch 78: training loss = 0.6223309269551479\n",
      "Training Accuracy after Epoch 78 : 87.34%\n",
      "Epoch 79: training loss = 0.6183348815557873\n",
      "Training Accuracy after Epoch 79 : 87.41%\n",
      "Epoch 80: training loss = 0.6144187990378402\n",
      "Training Accuracy after Epoch 80 : 87.47%\n",
      "Epoch 81: training loss = 0.6105738236530778\n",
      "Training Accuracy after Epoch 81 : 87.5%\n",
      "Epoch 82: training loss = 0.6066778408017164\n",
      "Training Accuracy after Epoch 82 : 87.64%\n",
      "Epoch 83: training loss = 0.6029072311580975\n",
      "Training Accuracy after Epoch 83 : 87.62%\n",
      "Epoch 84: training loss = 0.5992119621772025\n",
      "Training Accuracy after Epoch 84 : 87.77%\n",
      "Epoch 85: training loss = 0.5955945888070194\n",
      "Training Accuracy after Epoch 85 : 87.73%\n",
      "Epoch 86: training loss = 0.59204630615164\n",
      "Training Accuracy after Epoch 86 : 87.84%\n",
      "Epoch 87: training loss = 0.5885534492576806\n",
      "Training Accuracy after Epoch 87 : 87.83%\n",
      "Epoch 88: training loss = 0.585106804659665\n",
      "Training Accuracy after Epoch 88 : 87.95%\n",
      "Epoch 89: training loss = 0.5817182157485984\n",
      "Training Accuracy after Epoch 89 : 87.91%\n",
      "Epoch 90: training loss = 0.5783884304847294\n",
      "Training Accuracy after Epoch 90 : 88.02%\n",
      "Epoch 91: training loss = 0.5751182808447831\n",
      "Training Accuracy after Epoch 91 : 87.98%\n",
      "Epoch 92: training loss = 0.5719070521940842\n",
      "Training Accuracy after Epoch 92 : 88.11%\n",
      "Epoch 93: training loss = 0.5687102334082096\n",
      "Training Accuracy after Epoch 93 : 88.08%\n",
      "Epoch 94: training loss = 0.5656239544734898\n",
      "Training Accuracy after Epoch 94 : 88.19%\n",
      "Epoch 95: training loss = 0.5625439597264426\n",
      "Training Accuracy after Epoch 95 : 88.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: training loss = 0.559565952525091\n",
      "Training Accuracy after Epoch 96 : 88.28%\n",
      "Epoch 97: training loss = 0.5566153424686655\n",
      "Training Accuracy after Epoch 97 : 88.3%\n",
      "Epoch 98: training loss = 0.553722304632462\n",
      "Training Accuracy after Epoch 98 : 88.38%\n",
      "Epoch 99: training loss = 0.5508912120832595\n",
      "Training Accuracy after Epoch 99 : 88.37%\n",
      "Epoch 100: training loss = 0.5480671451579768\n",
      "Training Accuracy after Epoch 100 : 88.48%\n",
      "Epoch 101: training loss = 0.5453312263588012\n",
      "Training Accuracy after Epoch 101 : 88.47%\n",
      "Epoch 102: training loss = 0.5425965423218181\n",
      "Training Accuracy after Epoch 102 : 88.58%\n",
      "Epoch 103: training loss = 0.5399410902580619\n",
      "Training Accuracy after Epoch 103 : 88.54%\n",
      "Epoch 104: training loss = 0.5372736120919948\n",
      "Training Accuracy after Epoch 104 : 88.65%\n",
      "Epoch 105: training loss = 0.5347442966441253\n",
      "Training Accuracy after Epoch 105 : 88.64%\n",
      "Epoch 106: training loss = 0.5321821483305256\n",
      "Training Accuracy after Epoch 106 : 88.75%\n",
      "Epoch 107: training loss = 0.5297455359849327\n",
      "Training Accuracy after Epoch 107 : 88.7%\n",
      "Epoch 108: training loss = 0.5272554393775565\n",
      "Training Accuracy after Epoch 108 : 88.79%\n",
      "Epoch 109: training loss = 0.5248715101498013\n",
      "Training Accuracy after Epoch 109 : 88.77%\n",
      "Epoch 110: training loss = 0.5224433819909746\n",
      "Training Accuracy after Epoch 110 : 88.86%\n",
      "Epoch 111: training loss = 0.5201163180055595\n",
      "Training Accuracy after Epoch 111 : 88.85%\n",
      "Epoch 112: training loss = 0.5177635364294118\n",
      "Training Accuracy after Epoch 112 : 88.94%\n",
      "Epoch 113: training loss = 0.5154860991965193\n",
      "Training Accuracy after Epoch 113 : 88.92%\n",
      "Epoch 114: training loss = 0.5132131398524539\n",
      "Training Accuracy after Epoch 114 : 89.02%\n",
      "Epoch 115: training loss = 0.5110151253908594\n",
      "Training Accuracy after Epoch 115 : 88.97%\n",
      "Epoch 116: training loss = 0.5088282874772886\n",
      "Training Accuracy after Epoch 116 : 89.06%\n",
      "Epoch 117: training loss = 0.5067283268398046\n",
      "Training Accuracy after Epoch 117 : 89.03%\n",
      "Epoch 118: training loss = 0.5046851578200672\n",
      "Training Accuracy after Epoch 118 : 89.14%\n",
      "Epoch 119: training loss = 0.5025266577282719\n",
      "Training Accuracy after Epoch 119 : 89.09%\n",
      "Epoch 120: training loss = 0.5004438717052186\n",
      "Training Accuracy after Epoch 120 : 89.21%\n",
      "Epoch 121: training loss = 0.4984127795197024\n",
      "Training Accuracy after Epoch 121 : 89.16%\n",
      "Epoch 122: training loss = 0.49643829720002475\n",
      "Training Accuracy after Epoch 122 : 89.28%\n",
      "Epoch 123: training loss = 0.494526370464457\n",
      "Training Accuracy after Epoch 123 : 89.22%\n",
      "Epoch 124: training loss = 0.49275327294262194\n",
      "Training Accuracy after Epoch 124 : 89.34%\n",
      "Epoch 125: training loss = 0.49060950384538304\n",
      "Training Accuracy after Epoch 125 : 89.25%\n",
      "Epoch 126: training loss = 0.48870633552438697\n",
      "Training Accuracy after Epoch 126 : 89.43%\n",
      "Epoch 127: training loss = 0.48677758594299625\n",
      "Training Accuracy after Epoch 127 : 89.32%\n",
      "Epoch 128: training loss = 0.48502899021530393\n",
      "Training Accuracy after Epoch 128 : 89.5%\n",
      "Epoch 129: training loss = 0.48337725568495954\n",
      "Training Accuracy after Epoch 129 : 89.37%\n",
      "Epoch 130: training loss = 0.48155349542003284\n",
      "Training Accuracy after Epoch 130 : 89.54%\n",
      "Epoch 131: training loss = 0.4798703141319224\n",
      "Training Accuracy after Epoch 131 : 89.45%\n",
      "Epoch 132: training loss = 0.4779983387469626\n",
      "Training Accuracy after Epoch 132 : 89.58%\n",
      "Epoch 133: training loss = 0.47626349268962986\n",
      "Training Accuracy after Epoch 133 : 89.51%\n",
      "Epoch 134: training loss = 0.4746158510878422\n",
      "Training Accuracy after Epoch 134 : 89.62%\n",
      "Epoch 135: training loss = 0.4728229014885685\n",
      "Training Accuracy after Epoch 135 : 89.57%\n",
      "Epoch 136: training loss = 0.47113780953377177\n",
      "Training Accuracy after Epoch 136 : 89.68%\n",
      "Epoch 137: training loss = 0.4693684216311585\n",
      "Training Accuracy after Epoch 137 : 89.61%\n",
      "Epoch 138: training loss = 0.4677503253754617\n",
      "Training Accuracy after Epoch 138 : 89.75%\n",
      "Epoch 139: training loss = 0.4660339623944359\n",
      "Training Accuracy after Epoch 139 : 89.66%\n",
      "Epoch 140: training loss = 0.46446941110075224\n",
      "Training Accuracy after Epoch 140 : 89.78%\n",
      "Epoch 141: training loss = 0.4628108906307301\n",
      "Training Accuracy after Epoch 141 : 89.73%\n",
      "Epoch 142: training loss = 0.4613529861510906\n",
      "Training Accuracy after Epoch 142 : 89.82%\n",
      "Epoch 143: training loss = 0.4597220455929739\n",
      "Training Accuracy after Epoch 143 : 89.78%\n",
      "Epoch 144: training loss = 0.45832776367493033\n",
      "Training Accuracy after Epoch 144 : 89.87%\n",
      "Epoch 145: training loss = 0.45673887666848517\n",
      "Training Accuracy after Epoch 145 : 89.85%\n",
      "Epoch 146: training loss = 0.45532064155216495\n",
      "Training Accuracy after Epoch 146 : 89.93%\n",
      "Epoch 147: training loss = 0.4537411963683196\n",
      "Training Accuracy after Epoch 147 : 89.9%\n",
      "Epoch 148: training loss = 0.45233631155597936\n",
      "Training Accuracy after Epoch 148 : 89.96%\n",
      "Epoch 149: training loss = 0.4508101164382897\n",
      "Training Accuracy after Epoch 149 : 89.94%\n",
      "Epoch 150: training loss = 0.4494371635897606\n",
      "Training Accuracy after Epoch 150 : 89.98%\n",
      "Epoch 151: training loss = 0.44797076096995014\n",
      "Training Accuracy after Epoch 151 : 89.97%\n",
      "Epoch 152: training loss = 0.4466458112442467\n",
      "Training Accuracy after Epoch 152 : 90.03%\n",
      "Epoch 153: training loss = 0.44525883787402243\n",
      "Training Accuracy after Epoch 153 : 90.01%\n",
      "Epoch 154: training loss = 0.44398023487807114\n",
      "Training Accuracy after Epoch 154 : 90.1%\n",
      "Epoch 155: training loss = 0.4426440616622764\n",
      "Training Accuracy after Epoch 155 : 90.04%\n",
      "Epoch 156: training loss = 0.4413364753983661\n",
      "Training Accuracy after Epoch 156 : 90.14%\n",
      "Epoch 157: training loss = 0.4399655657041581\n",
      "Training Accuracy after Epoch 157 : 90.08%\n",
      "Epoch 158: training loss = 0.43864363093329717\n",
      "Training Accuracy after Epoch 158 : 90.18%\n",
      "Epoch 159: training loss = 0.43732199647708714\n",
      "Training Accuracy after Epoch 159 : 90.13%\n",
      "Epoch 160: training loss = 0.4360567947452926\n",
      "Training Accuracy after Epoch 160 : 90.22%\n",
      "Epoch 161: training loss = 0.43473818161433697\n",
      "Training Accuracy after Epoch 161 : 90.17%\n",
      "Epoch 162: training loss = 0.4335133767549008\n",
      "Training Accuracy after Epoch 162 : 90.26%\n",
      "Epoch 163: training loss = 0.43228906287127433\n",
      "Training Accuracy after Epoch 163 : 90.2%\n",
      "Epoch 164: training loss = 0.4310448098961478\n",
      "Training Accuracy after Epoch 164 : 90.29%\n",
      "Epoch 165: training loss = 0.4298604249404699\n",
      "Training Accuracy after Epoch 165 : 90.23%\n",
      "Epoch 166: training loss = 0.42865319709816396\n",
      "Training Accuracy after Epoch 166 : 90.3%\n",
      "Epoch 167: training loss = 0.42743829388329985\n",
      "Training Accuracy after Epoch 167 : 90.27%\n",
      "Epoch 168: training loss = 0.4263242967204453\n",
      "Training Accuracy after Epoch 168 : 90.34%\n",
      "Epoch 169: training loss = 0.42519800761067145\n",
      "Training Accuracy after Epoch 169 : 90.28%\n",
      "Epoch 170: training loss = 0.4239513156944273\n",
      "Training Accuracy after Epoch 170 : 90.39%\n",
      "Epoch 171: training loss = 0.42270723810239663\n",
      "Training Accuracy after Epoch 171 : 90.32%\n",
      "Epoch 172: training loss = 0.4215838483299693\n",
      "Training Accuracy after Epoch 172 : 90.41%\n",
      "Epoch 173: training loss = 0.42049243487543836\n",
      "Training Accuracy after Epoch 173 : 90.36%\n",
      "Epoch 174: training loss = 0.41928701284024583\n",
      "Training Accuracy after Epoch 174 : 90.45%\n",
      "Epoch 175: training loss = 0.4181564963666175\n",
      "Training Accuracy after Epoch 175 : 90.42%\n",
      "Epoch 176: training loss = 0.41710976140262424\n",
      "Training Accuracy after Epoch 176 : 90.49%\n",
      "Epoch 177: training loss = 0.41612119342126913\n",
      "Training Accuracy after Epoch 177 : 90.43%\n",
      "Epoch 178: training loss = 0.4149308593100108\n",
      "Training Accuracy after Epoch 178 : 90.51%\n",
      "Epoch 179: training loss = 0.4138000345823977\n",
      "Training Accuracy after Epoch 179 : 90.45%\n",
      "Epoch 180: training loss = 0.4128790959686898\n",
      "Training Accuracy after Epoch 180 : 90.55%\n",
      "Epoch 181: training loss = 0.4118720432225514\n",
      "Training Accuracy after Epoch 181 : 90.46%\n",
      "Epoch 182: training loss = 0.4110329473437742\n",
      "Training Accuracy after Epoch 182 : 90.56%\n",
      "Epoch 183: training loss = 0.4101154296492332\n",
      "Training Accuracy after Epoch 183 : 90.48%\n",
      "Epoch 184: training loss = 0.4091351060684402\n",
      "Training Accuracy after Epoch 184 : 90.57%\n",
      "Epoch 185: training loss = 0.40806669481557334\n",
      "Training Accuracy after Epoch 185 : 90.52%\n",
      "Epoch 186: training loss = 0.4071550606302647\n",
      "Training Accuracy after Epoch 186 : 90.6%\n",
      "Epoch 187: training loss = 0.40609470392000174\n",
      "Training Accuracy after Epoch 187 : 90.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188: training loss = 0.40516723936708043\n",
      "Training Accuracy after Epoch 188 : 90.65%\n",
      "Epoch 189: training loss = 0.4041432292605096\n",
      "Training Accuracy after Epoch 189 : 90.61%\n",
      "Epoch 190: training loss = 0.40309953592698333\n",
      "Training Accuracy after Epoch 190 : 90.68%\n",
      "Epoch 191: training loss = 0.40202719847243445\n",
      "Training Accuracy after Epoch 191 : 90.66%\n",
      "Epoch 192: training loss = 0.40104681173791373\n",
      "Training Accuracy after Epoch 192 : 90.7%\n",
      "Epoch 193: training loss = 0.39998513087951776\n",
      "Training Accuracy after Epoch 193 : 90.7%\n",
      "Epoch 194: training loss = 0.3990645815253937\n",
      "Training Accuracy after Epoch 194 : 90.72%\n",
      "Epoch 195: training loss = 0.39802396476540225\n",
      "Training Accuracy after Epoch 195 : 90.72%\n",
      "Epoch 196: training loss = 0.39716791143297436\n",
      "Training Accuracy after Epoch 196 : 90.77%\n",
      "Epoch 197: training loss = 0.39617989260492475\n",
      "Training Accuracy after Epoch 197 : 90.76%\n",
      "Epoch 198: training loss = 0.3953509784914246\n",
      "Training Accuracy after Epoch 198 : 90.79%\n",
      "Epoch 199: training loss = 0.3944862610305644\n",
      "Training Accuracy after Epoch 199 : 90.78%\n",
      "Epoch 200: training loss = 0.39390195641185044\n",
      "Training Accuracy after Epoch 200 : 90.82%\n",
      "Epoch 201: training loss = 0.3929183332313559\n",
      "Training Accuracy after Epoch 201 : 90.81%\n",
      "Epoch 202: training loss = 0.39232035377425806\n",
      "Training Accuracy after Epoch 202 : 90.83%\n",
      "Epoch 203: training loss = 0.39126122040713723\n",
      "Training Accuracy after Epoch 203 : 90.82%\n",
      "Epoch 204: training loss = 0.39055148350135316\n",
      "Training Accuracy after Epoch 204 : 90.85%\n",
      "Epoch 205: training loss = 0.38948258130248703\n",
      "Training Accuracy after Epoch 205 : 90.85%\n",
      "Epoch 206: training loss = 0.3887064096076058\n",
      "Training Accuracy after Epoch 206 : 90.88%\n",
      "Epoch 207: training loss = 0.38763483041615343\n",
      "Training Accuracy after Epoch 207 : 90.87%\n",
      "Epoch 208: training loss = 0.3868505747565661\n",
      "Training Accuracy after Epoch 208 : 90.9%\n",
      "Epoch 209: training loss = 0.3858035135605388\n",
      "Training Accuracy after Epoch 209 : 90.91%\n",
      "Epoch 210: training loss = 0.38506598443625834\n",
      "Training Accuracy after Epoch 210 : 90.94%\n",
      "Epoch 211: training loss = 0.38406818283802135\n",
      "Training Accuracy after Epoch 211 : 90.95%\n",
      "Epoch 212: training loss = 0.3834206400101907\n",
      "Training Accuracy after Epoch 212 : 90.96%\n",
      "Epoch 213: training loss = 0.3824678945125438\n",
      "Training Accuracy after Epoch 213 : 90.99%\n",
      "Epoch 214: training loss = 0.38190318082561714\n",
      "Training Accuracy after Epoch 214 : 90.99%\n",
      "Epoch 215: training loss = 0.3809881961044366\n",
      "Training Accuracy after Epoch 215 : 91.0%\n",
      "Epoch 216: training loss = 0.380464746129623\n",
      "Training Accuracy after Epoch 216 : 91.01%\n",
      "Epoch 217: training loss = 0.37958976198255673\n",
      "Training Accuracy after Epoch 217 : 91.02%\n",
      "Epoch 218: training loss = 0.3790118462057068\n",
      "Training Accuracy after Epoch 218 : 91.01%\n",
      "Epoch 219: training loss = 0.37812696820832986\n",
      "Training Accuracy after Epoch 219 : 91.02%\n",
      "Epoch 220: training loss = 0.37750515335128154\n",
      "Training Accuracy after Epoch 220 : 91.04%\n",
      "Epoch 221: training loss = 0.3765839761967972\n",
      "Training Accuracy after Epoch 221 : 91.07%\n",
      "Epoch 222: training loss = 0.37591347370844347\n",
      "Training Accuracy after Epoch 222 : 91.08%\n",
      "Epoch 223: training loss = 0.3749735715171782\n",
      "Training Accuracy after Epoch 223 : 91.08%\n",
      "Epoch 224: training loss = 0.37427025601307007\n",
      "Training Accuracy after Epoch 224 : 91.13%\n",
      "Epoch 225: training loss = 0.37338510123327345\n",
      "Training Accuracy after Epoch 225 : 91.13%\n",
      "Epoch 226: training loss = 0.3726900828326038\n",
      "Training Accuracy after Epoch 226 : 91.17%\n",
      "Epoch 227: training loss = 0.37182867602296604\n",
      "Training Accuracy after Epoch 227 : 91.18%\n",
      "Epoch 228: training loss = 0.37117753560250594\n",
      "Training Accuracy after Epoch 228 : 91.18%\n",
      "Epoch 229: training loss = 0.37035622774212207\n",
      "Training Accuracy after Epoch 229 : 91.2%\n",
      "Epoch 230: training loss = 0.369757940396484\n",
      "Training Accuracy after Epoch 230 : 91.2%\n",
      "Epoch 231: training loss = 0.36899159168167156\n",
      "Training Accuracy after Epoch 231 : 91.21%\n",
      "Epoch 232: training loss = 0.36843028859005084\n",
      "Training Accuracy after Epoch 232 : 91.23%\n",
      "Epoch 233: training loss = 0.36771890524214557\n",
      "Training Accuracy after Epoch 233 : 91.23%\n",
      "Epoch 234: training loss = 0.3671642441464937\n",
      "Training Accuracy after Epoch 234 : 91.25%\n",
      "Epoch 235: training loss = 0.3664973355381538\n",
      "Training Accuracy after Epoch 235 : 91.25%\n",
      "Epoch 236: training loss = 0.3658877574277856\n",
      "Training Accuracy after Epoch 236 : 91.27%\n",
      "Epoch 237: training loss = 0.3652308147087057\n",
      "Training Accuracy after Epoch 237 : 91.26%\n",
      "Epoch 238: training loss = 0.36451742463864945\n",
      "Training Accuracy after Epoch 238 : 91.3%\n",
      "Epoch 239: training loss = 0.36376209642432716\n",
      "Training Accuracy after Epoch 239 : 91.28%\n",
      "Epoch 240: training loss = 0.3630611286816785\n",
      "Training Accuracy after Epoch 240 : 91.34%\n",
      "Epoch 241: training loss = 0.3623317915969342\n",
      "Training Accuracy after Epoch 241 : 91.3%\n",
      "Epoch 242: training loss = 0.36163621283225017\n",
      "Training Accuracy after Epoch 242 : 91.36%\n",
      "Epoch 243: training loss = 0.3609147723965528\n",
      "Training Accuracy after Epoch 243 : 91.34%\n",
      "Epoch 244: training loss = 0.36024910559823425\n",
      "Training Accuracy after Epoch 244 : 91.38%\n",
      "Epoch 245: training loss = 0.3595821474999147\n",
      "Training Accuracy after Epoch 245 : 91.37%\n",
      "Epoch 246: training loss = 0.35897632051771156\n",
      "Training Accuracy after Epoch 246 : 91.41%\n",
      "Epoch 247: training loss = 0.3583401102639402\n",
      "Training Accuracy after Epoch 247 : 91.4%\n",
      "Epoch 248: training loss = 0.35778950711819035\n",
      "Training Accuracy after Epoch 248 : 91.44%\n",
      "Epoch 249: training loss = 0.35722768164646335\n",
      "Training Accuracy after Epoch 249 : 91.39%\n",
      "Epoch 250: training loss = 0.35663260137079333\n",
      "Training Accuracy after Epoch 250 : 91.44%\n",
      "Epoch 251: training loss = 0.3560422454635833\n",
      "Training Accuracy after Epoch 251 : 91.41%\n",
      "Epoch 252: training loss = 0.3554835323408165\n",
      "Training Accuracy after Epoch 252 : 91.46%\n",
      "Epoch 253: training loss = 0.35483413020743093\n",
      "Training Accuracy after Epoch 253 : 91.42%\n",
      "Epoch 254: training loss = 0.3542741739165709\n",
      "Training Accuracy after Epoch 254 : 91.46%\n",
      "Epoch 255: training loss = 0.3537315502312594\n",
      "Training Accuracy after Epoch 255 : 91.42%\n",
      "Epoch 256: training loss = 0.3530463261942518\n",
      "Training Accuracy after Epoch 256 : 91.5%\n",
      "Epoch 257: training loss = 0.35243454832759225\n",
      "Training Accuracy after Epoch 257 : 91.45%\n",
      "Epoch 258: training loss = 0.35184115232886803\n",
      "Training Accuracy after Epoch 258 : 91.52%\n",
      "Epoch 259: training loss = 0.3512300300753275\n",
      "Training Accuracy after Epoch 259 : 91.48%\n",
      "Epoch 260: training loss = 0.350731989539342\n",
      "Training Accuracy after Epoch 260 : 91.54%\n",
      "Epoch 261: training loss = 0.35020729144991397\n",
      "Training Accuracy after Epoch 261 : 91.5%\n",
      "Epoch 262: training loss = 0.34951210391990517\n",
      "Training Accuracy after Epoch 262 : 91.55%\n",
      "Epoch 263: training loss = 0.3488362681282742\n",
      "Training Accuracy after Epoch 263 : 91.53%\n",
      "Epoch 264: training loss = 0.3481673536326683\n",
      "Training Accuracy after Epoch 264 : 91.57%\n",
      "Epoch 265: training loss = 0.34752805404946174\n",
      "Training Accuracy after Epoch 265 : 91.57%\n",
      "Epoch 266: training loss = 0.34703480742368575\n",
      "Training Accuracy after Epoch 266 : 91.6%\n",
      "Epoch 267: training loss = 0.3466121287484266\n",
      "Training Accuracy after Epoch 267 : 91.56%\n",
      "Epoch 268: training loss = 0.34587013106362113\n",
      "Training Accuracy after Epoch 268 : 91.61%\n",
      "Epoch 269: training loss = 0.3453585875078739\n",
      "Training Accuracy after Epoch 269 : 91.61%\n",
      "Epoch 270: training loss = 0.3448822694974577\n",
      "Training Accuracy after Epoch 270 : 91.64%\n",
      "Epoch 271: training loss = 0.3444651080865594\n",
      "Training Accuracy after Epoch 271 : 91.61%\n",
      "Epoch 272: training loss = 0.34383695499831723\n",
      "Training Accuracy after Epoch 272 : 91.66%\n",
      "Epoch 273: training loss = 0.343353746418931\n",
      "Training Accuracy after Epoch 273 : 91.63%\n",
      "Epoch 274: training loss = 0.342766537211983\n",
      "Training Accuracy after Epoch 274 : 91.66%\n",
      "Epoch 275: training loss = 0.34232464020153386\n",
      "Training Accuracy after Epoch 275 : 91.66%\n",
      "Epoch 276: training loss = 0.3417420896643024\n",
      "Training Accuracy after Epoch 276 : 91.68%\n",
      "Epoch 277: training loss = 0.341262515982804\n",
      "Training Accuracy after Epoch 277 : 91.68%\n",
      "Epoch 278: training loss = 0.3408103431293838\n",
      "Training Accuracy after Epoch 278 : 91.69%\n",
      "Epoch 279: training loss = 0.34039900704159315\n",
      "Training Accuracy after Epoch 279 : 91.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280: training loss = 0.33973101471909434\n",
      "Training Accuracy after Epoch 280 : 91.72%\n",
      "Epoch 281: training loss = 0.33925142864087043\n",
      "Training Accuracy after Epoch 281 : 91.69%\n",
      "Epoch 282: training loss = 0.338598750282509\n",
      "Training Accuracy after Epoch 282 : 91.74%\n",
      "Epoch 283: training loss = 0.33803722968904737\n",
      "Training Accuracy after Epoch 283 : 91.73%\n",
      "Epoch 284: training loss = 0.33748716450794997\n",
      "Training Accuracy after Epoch 284 : 91.77%\n",
      "Epoch 285: training loss = 0.3370515510543793\n",
      "Training Accuracy after Epoch 285 : 91.74%\n",
      "Epoch 286: training loss = 0.3363509661779879\n",
      "Training Accuracy after Epoch 286 : 91.78%\n",
      "Epoch 287: training loss = 0.3359041267652244\n",
      "Training Accuracy after Epoch 287 : 91.76%\n",
      "Epoch 288: training loss = 0.33532395517997926\n",
      "Training Accuracy after Epoch 288 : 91.79%\n",
      "Epoch 289: training loss = 0.3348811101153694\n",
      "Training Accuracy after Epoch 289 : 91.77%\n",
      "Epoch 290: training loss = 0.33450582953796604\n",
      "Training Accuracy after Epoch 290 : 91.81%\n",
      "Epoch 291: training loss = 0.3341949635492904\n",
      "Training Accuracy after Epoch 291 : 91.76%\n",
      "Epoch 292: training loss = 0.3337439321125811\n",
      "Training Accuracy after Epoch 292 : 91.83%\n",
      "Epoch 293: training loss = 0.33335963231594967\n",
      "Training Accuracy after Epoch 293 : 91.77%\n",
      "Epoch 294: training loss = 0.33290374956332736\n",
      "Training Accuracy after Epoch 294 : 91.83%\n",
      "Epoch 295: training loss = 0.33240795931885764\n",
      "Training Accuracy after Epoch 295 : 91.8%\n",
      "Epoch 296: training loss = 0.3319063854067076\n",
      "Training Accuracy after Epoch 296 : 91.84%\n",
      "Epoch 297: training loss = 0.33136882377390403\n",
      "Training Accuracy after Epoch 297 : 91.82%\n",
      "Epoch 298: training loss = 0.3306999436639619\n",
      "Training Accuracy after Epoch 298 : 91.88%\n",
      "Epoch 299: training loss = 0.3301942661906393\n",
      "Training Accuracy after Epoch 299 : 91.85%\n",
      "Epoch 300: training loss = 0.32964476011243393\n",
      "Training Accuracy after Epoch 300 : 91.89%\n"
     ]
    }
   ],
   "source": [
    "W2,b2,W1,b1=training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model on Testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(W2,b2,W1,b1,X_test,y_test):\n",
    "    \n",
    "    dotprod1, h1, dotprod2, output=feedforward(X_test,W1,b1,W2,b2)   \n",
    "    labely_test=y_test\n",
    "    #np.argmax(y_test,axis=1)\n",
    "    labelpred=np.argmax(output,axis=1)\n",
    "    accuracytest = round((accuracy_score(labely_test, labelpred)*100),2)        \n",
    "    print(\"Accuracy on Testing data : {}%\".format(accuracytest))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on Testing data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing data : 90.87%\n"
     ]
    }
   ],
   "source": [
    "result=testing(W2,b2,W1,b1,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
