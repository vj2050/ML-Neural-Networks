{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAISHNAVI JAMDADE(TM39453)\n",
    "### HOMEWORK 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the file into numpy array of dimensions (60000,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertfunc(file):\n",
    "    arr = idx2numpy.convert_from_file(file)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=convertfunc('train-images.idx3-ubyte')\n",
    "y_train=convertfunc('train-labels.idx1-ubyte')\n",
    "X_test=convertfunc('t10k-images.idx3-ubyte')\n",
    "y_test=convertfunc('t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapefunc(array,rows,columns):\n",
    "    array=array.reshape(rows,columns)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=reshapefunc(X_train,60000,784)\n",
    "X_test=reshapefunc(X_test,10000,784)\n",
    "#y_train=reshapefunc(y_train,1,60000)\n",
    "#y_test=reshapefunc(y_test,1,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape, y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Training data into Training and Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Question 3, Here we have split the original X_train(training dataset) into X_trainval and X_testval where we will train our model with 4 configurations on our X_trainval data, and then evaluate it on our Validation dataset that is, X_testval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval,X_testval,y_trainval,y_testval=train_test_split(X_train,y_train,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (18000, 784), (42000,), (18000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainval.shape, X_testval.shape, y_trainval.shape,y_testval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One hot label encoding for output labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using one hot label encoding for validation output labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainval = pd.get_dummies(y_trainval)\n",
    "y_trainval=np.array(y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(inputs, hNeurons, oNeurons):\n",
    "\n",
    "    weights1=np.random.randn(inputs,hNeurons)*np.sqrt(1./inputs)    #784 rows 5 columns\n",
    "    bias1=np.zeros((1, hNeurons))*np.sqrt(1./inputs)\n",
    "    #hence x.w = 1,5 dimensions + bias [1,5]\n",
    "    weights2=np.random.randn(hNeurons,oNeurons)*np.sqrt(1./hNeurons)\n",
    "    #hence x.w= 5,10 dimensions + bias [1,10]\n",
    "    bias2= np.zeros((1, oNeurons))*np.sqrt(1./hNeurons)\n",
    "\n",
    "    #ouptut vecotr is going to be of dimensions [1,10] then softmax\n",
    "    #W1.shape , b1.shape, W2.shape, b2.shape\n",
    "    \n",
    "    return weights1, bias1, weights2, bias2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    act = 1. / (1. + np.exp(-z))\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising the values:\n",
    "\n",
    "def softmax(z):\n",
    "    softact = np.exp(z)/np.sum(np.exp(z), axis=1,keepdims=True)                                       \n",
    "    return softact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(X_t,W1,b1,W2,b2):\n",
    "    z1= np.dot(X_t,W1)+b1\n",
    "    h1=sigmoid(z1)\n",
    "    \n",
    "    z2=np.dot(h1,W2)+b2\n",
    "    output=softmax(z2)\n",
    "    return z1, h1, z2, output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(ypred,y_t):     \n",
    "    n_samples = y_t.shape[0]\n",
    "    L = (ypred-y_t)/n_samples\n",
    "    return L    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(ypred,y_t):\n",
    "    L_sum = np.sum(np.multiply(y_t, np.log(ypred)))\n",
    "    num_samples = y_t.shape[0]\n",
    "    L = -(1./num_samples) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate Sigmoid Derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(h1):\n",
    "    deriv=h1*(1-h1)\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Function to compute Gradient Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(h1,W2,ypred,y_t,X_t):\n",
    "    \n",
    "    #Calculate error\n",
    "    L=error(ypred,y_t)\n",
    "    \n",
    "    # z1, h1, z2, output\n",
    "    # z1, and z2 => dot products\n",
    "    # h1 and output => activations\n",
    "    change_output=L\n",
    "    #dL/dW2\n",
    "    delta_W2= np.dot(h1.T,change_output)\n",
    "    \n",
    "    #dL/db2\n",
    "    delta_b2= np.sum(change_output,axis=0,keepdims=True)\n",
    "    \n",
    "    #dL/dh1\n",
    "    delta_h1=np.dot(change_output,W2.T)\n",
    "\n",
    "    change_h1= delta_h1*sigmoid_derivative(h1)\n",
    "    \n",
    "    #dL/dW1\n",
    "    delta_W1= np.dot(X_t.T, change_h1)\n",
    "    \n",
    "    #dL/db1\n",
    "    delta_b1= np.sum(change_h1,axis=0,keepdims=True)\n",
    "    \n",
    "    return delta_W2, delta_b2, delta_W1, delta_b1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(alpha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1):\n",
    "    W2= W2- alpha*(delta_W2)\n",
    "    b2= b2 - alpha*(delta_b2)\n",
    "    W1= W1- alpha*(delta_W1)\n",
    "    b1=b1- alpha*(delta_b1)\n",
    "    \n",
    "    return W2,b2,W1,b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X_t,y_t,neurons,lr,iterations):\n",
    "    hNeurons=neurons                              #hidden neuron\n",
    "    inputs=X_t.shape[1]                   #number of inputs\n",
    "    oNeurons=y_t.shape[1]\n",
    "    alpha= lr                       #learning rate\n",
    "    epochs=iterations\n",
    "    \n",
    "    #retriving the actual labels for y_train:\n",
    "    labely_train=np.argmax(y_t,axis=1)\n",
    "    \n",
    "    #initialize weights\n",
    "    W1, b1, W2, b2= initialize_weights(inputs,hNeurons,oNeurons)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #feed forward\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_t,W1,b1,W2,b2)\n",
    "\n",
    "        #back propogate\n",
    "        delta_W2, delta_b2, delta_W1, delta_b1= backpropagation(h1,W2,output,y_t,X_t)\n",
    "        \n",
    "        #update weights using gradient descent\n",
    "        W2,b2,W1,b1=update_weights(alpha,W1,b1,W2,b2,delta_W2,delta_b2,delta_W1,delta_b1)\n",
    "\n",
    "        #again feed forward using updated weights\n",
    "        dotprod1, h1, dotprod2, output=feedforward(X_t,W1,b1,W2,b2)\n",
    "\n",
    "        #Calculating Total Loss\n",
    "        loss=crossentropy(output,y_t)                 # put in training func later\n",
    "        print(\"Epoch {}: training loss = {}\".format(i + 1,loss))\n",
    "        \n",
    "\n",
    "        #Retrieving the corresponding class labels:\n",
    "        labelpred=np.argmax(output,axis=1)\n",
    "        \n",
    "        #Training Accuracy:\n",
    "        accuracy = round((accuracy_score(labely_train, labelpred)*100),2)\n",
    "        print(\"Training Accuracy after Epoch {} : {}%\".format(i+1, accuracy))\n",
    "        \n",
    "    return W2,b2,W1,b1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(W2,b2,W1,b1,X_t,y_t):\n",
    "    \n",
    "    dotprod1, h1, dotprod2, output=feedforward(X_t,W1,b1,W2,b2)   \n",
    "    labely_test=y_t\n",
    "    labelpred=np.argmax(output,axis=1)\n",
    "    #accuracytest = round((accuracy_score(labely_test, labelpred)*100),2)        \n",
    "    #print(\"Accuracy on Testing data : {}%\".format(accuracytest))\n",
    "    return labely_test,labelpred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to measure accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labely_test, labelpred):\n",
    "    accuracytest = round((accuracy_score(labely_test, labelpred)*100),2)        \n",
    "    #print(\"Accuracy : {}%\".format(accuracytest))\n",
    "    return accuracytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 1 :\n",
    "* Learning rate = 0.01\n",
    "* Number of neurons = 128\n",
    "* Iteratations =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.4517242899731873\n",
      "Training Accuracy after Epoch 1 : 11.18%\n",
      "Epoch 2: training loss = 2.4135126899032304\n",
      "Training Accuracy after Epoch 2 : 11.95%\n",
      "Epoch 3: training loss = 2.3778329151156643\n",
      "Training Accuracy after Epoch 3 : 12.77%\n",
      "Epoch 4: training loss = 2.343561890589584\n",
      "Training Accuracy after Epoch 4 : 13.92%\n",
      "Epoch 5: training loss = 2.3114409968589125\n",
      "Training Accuracy after Epoch 5 : 15.1%\n",
      "Epoch 6: training loss = 2.281062432146635\n",
      "Training Accuracy after Epoch 6 : 16.31%\n",
      "Epoch 7: training loss = 2.2518189143622425\n",
      "Training Accuracy after Epoch 7 : 17.86%\n",
      "Epoch 8: training loss = 2.2241898628295833\n",
      "Training Accuracy after Epoch 8 : 19.37%\n",
      "Epoch 9: training loss = 2.197807278531044\n",
      "Training Accuracy after Epoch 9 : 20.96%\n",
      "Epoch 10: training loss = 2.1722232291726744\n",
      "Training Accuracy after Epoch 10 : 22.64%\n",
      "Epoch 11: training loss = 2.147833475981197\n",
      "Training Accuracy after Epoch 11 : 24.42%\n",
      "Epoch 12: training loss = 2.1244679846210976\n",
      "Training Accuracy after Epoch 12 : 26.22%\n",
      "Epoch 13: training loss = 2.1018758961568436\n",
      "Training Accuracy after Epoch 13 : 28.05%\n",
      "Epoch 14: training loss = 2.079774977987658\n",
      "Training Accuracy after Epoch 14 : 29.76%\n",
      "Epoch 15: training loss = 2.0581813267000038\n",
      "Training Accuracy after Epoch 15 : 31.42%\n",
      "Epoch 16: training loss = 2.036989671307302\n",
      "Training Accuracy after Epoch 16 : 33.16%\n",
      "Epoch 17: training loss = 2.0161406299524436\n",
      "Training Accuracy after Epoch 17 : 34.98%\n",
      "Epoch 18: training loss = 1.9960094439904168\n",
      "Training Accuracy after Epoch 18 : 36.5%\n",
      "Epoch 19: training loss = 1.9765876034162888\n",
      "Training Accuracy after Epoch 19 : 37.98%\n",
      "Epoch 20: training loss = 1.9573734528222393\n",
      "Training Accuracy after Epoch 20 : 39.42%\n",
      "Epoch 21: training loss = 1.9386920525384919\n",
      "Training Accuracy after Epoch 21 : 40.7%\n",
      "Epoch 22: training loss = 1.9207125347133167\n",
      "Training Accuracy after Epoch 22 : 41.96%\n",
      "Epoch 23: training loss = 1.9033794264747008\n",
      "Training Accuracy after Epoch 23 : 43.21%\n",
      "Epoch 24: training loss = 1.8863221559591299\n",
      "Training Accuracy after Epoch 24 : 44.51%\n",
      "Epoch 25: training loss = 1.8694999291968817\n",
      "Training Accuracy after Epoch 25 : 45.55%\n",
      "Epoch 26: training loss = 1.8531504122526918\n",
      "Training Accuracy after Epoch 26 : 46.62%\n",
      "Epoch 27: training loss = 1.8372802752332504\n",
      "Training Accuracy after Epoch 27 : 47.59%\n",
      "Epoch 28: training loss = 1.8217484567694864\n",
      "Training Accuracy after Epoch 28 : 48.59%\n",
      "Epoch 29: training loss = 1.806727541820578\n",
      "Training Accuracy after Epoch 29 : 49.58%\n",
      "Epoch 30: training loss = 1.7920999447843988\n",
      "Training Accuracy after Epoch 30 : 50.55%\n",
      "Epoch 31: training loss = 1.7777391537250036\n",
      "Training Accuracy after Epoch 31 : 51.49%\n",
      "Epoch 32: training loss = 1.7636512251376966\n",
      "Training Accuracy after Epoch 32 : 52.33%\n",
      "Epoch 33: training loss = 1.749814074772923\n",
      "Training Accuracy after Epoch 33 : 53.17%\n",
      "Epoch 34: training loss = 1.7362352558738703\n",
      "Training Accuracy after Epoch 34 : 53.98%\n",
      "Epoch 35: training loss = 1.7230179054167023\n",
      "Training Accuracy after Epoch 35 : 54.8%\n",
      "Epoch 36: training loss = 1.710139078033963\n",
      "Training Accuracy after Epoch 36 : 55.49%\n",
      "Epoch 37: training loss = 1.6975306935534706\n",
      "Training Accuracy after Epoch 37 : 56.13%\n",
      "Epoch 38: training loss = 1.6851535390372516\n",
      "Training Accuracy after Epoch 38 : 56.79%\n",
      "Epoch 39: training loss = 1.6731031614382956\n",
      "Training Accuracy after Epoch 39 : 57.42%\n",
      "Epoch 40: training loss = 1.6613656131295793\n",
      "Training Accuracy after Epoch 40 : 57.89%\n",
      "Epoch 41: training loss = 1.6497242425131755\n",
      "Training Accuracy after Epoch 41 : 58.46%\n",
      "Epoch 42: training loss = 1.6381781354455263\n",
      "Training Accuracy after Epoch 42 : 59.13%\n",
      "Epoch 43: training loss = 1.6267991113295697\n",
      "Training Accuracy after Epoch 43 : 59.68%\n",
      "Epoch 44: training loss = 1.6156465045462822\n",
      "Training Accuracy after Epoch 44 : 60.26%\n",
      "Epoch 45: training loss = 1.604674965527693\n",
      "Training Accuracy after Epoch 45 : 60.86%\n",
      "Epoch 46: training loss = 1.5939152573593875\n",
      "Training Accuracy after Epoch 46 : 61.42%\n",
      "Epoch 47: training loss = 1.5833517385351485\n",
      "Training Accuracy after Epoch 47 : 61.95%\n",
      "Epoch 48: training loss = 1.5729499924108947\n",
      "Training Accuracy after Epoch 48 : 62.42%\n",
      "Epoch 49: training loss = 1.5627220398356336\n",
      "Training Accuracy after Epoch 49 : 62.92%\n",
      "Epoch 50: training loss = 1.5526535881106134\n",
      "Training Accuracy after Epoch 50 : 63.3%\n",
      "Epoch 51: training loss = 1.5428380832211654\n",
      "Training Accuracy after Epoch 51 : 63.69%\n",
      "Epoch 52: training loss = 1.533232555962714\n",
      "Training Accuracy after Epoch 52 : 64.13%\n",
      "Epoch 53: training loss = 1.523706718817056\n",
      "Training Accuracy after Epoch 53 : 64.53%\n",
      "Epoch 54: training loss = 1.5141436161911777\n",
      "Training Accuracy after Epoch 54 : 64.87%\n",
      "Epoch 55: training loss = 1.5046417884158791\n",
      "Training Accuracy after Epoch 55 : 65.33%\n",
      "Epoch 56: training loss = 1.4952740091044896\n",
      "Training Accuracy after Epoch 56 : 65.75%\n",
      "Epoch 57: training loss = 1.486026016594631\n",
      "Training Accuracy after Epoch 57 : 66.05%\n",
      "Epoch 58: training loss = 1.4769046363557519\n",
      "Training Accuracy after Epoch 58 : 66.4%\n",
      "Epoch 59: training loss = 1.4679247439994443\n",
      "Training Accuracy after Epoch 59 : 66.76%\n",
      "Epoch 60: training loss = 1.4591783663685232\n",
      "Training Accuracy after Epoch 60 : 67.05%\n",
      "Epoch 61: training loss = 1.4506028500303454\n",
      "Training Accuracy after Epoch 61 : 67.35%\n",
      "Epoch 62: training loss = 1.4421925406266518\n",
      "Training Accuracy after Epoch 62 : 67.65%\n",
      "Epoch 63: training loss = 1.4338823716284483\n",
      "Training Accuracy after Epoch 63 : 67.99%\n",
      "Epoch 64: training loss = 1.425599535536051\n",
      "Training Accuracy after Epoch 64 : 68.31%\n",
      "Epoch 65: training loss = 1.4173793323109698\n",
      "Training Accuracy after Epoch 65 : 68.64%\n",
      "Epoch 66: training loss = 1.40921631513174\n",
      "Training Accuracy after Epoch 66 : 68.95%\n",
      "Epoch 67: training loss = 1.401059911848045\n",
      "Training Accuracy after Epoch 67 : 69.29%\n",
      "Epoch 68: training loss = 1.392977805494372\n",
      "Training Accuracy after Epoch 68 : 69.62%\n",
      "Epoch 69: training loss = 1.3851110312551078\n",
      "Training Accuracy after Epoch 69 : 69.91%\n",
      "Epoch 70: training loss = 1.3774151346789894\n",
      "Training Accuracy after Epoch 70 : 70.2%\n",
      "Epoch 71: training loss = 1.3698306544354462\n",
      "Training Accuracy after Epoch 71 : 70.47%\n",
      "Epoch 72: training loss = 1.3623596809593252\n",
      "Training Accuracy after Epoch 72 : 70.72%\n",
      "Epoch 73: training loss = 1.355028918567907\n",
      "Training Accuracy after Epoch 73 : 70.94%\n",
      "Epoch 74: training loss = 1.3477956542066045\n",
      "Training Accuracy after Epoch 74 : 71.23%\n",
      "Epoch 75: training loss = 1.3407029113625748\n",
      "Training Accuracy after Epoch 75 : 71.51%\n",
      "Epoch 76: training loss = 1.3337676503617957\n",
      "Training Accuracy after Epoch 76 : 71.72%\n",
      "Epoch 77: training loss = 1.3269950017948522\n",
      "Training Accuracy after Epoch 77 : 71.92%\n",
      "Epoch 78: training loss = 1.3203563262493683\n",
      "Training Accuracy after Epoch 78 : 72.2%\n",
      "Epoch 79: training loss = 1.313812361034995\n",
      "Training Accuracy after Epoch 79 : 72.46%\n",
      "Epoch 80: training loss = 1.3073124775252143\n",
      "Training Accuracy after Epoch 80 : 72.66%\n",
      "Epoch 81: training loss = 1.3008316954986934\n",
      "Training Accuracy after Epoch 81 : 72.85%\n",
      "Epoch 82: training loss = 1.2944173879679433\n",
      "Training Accuracy after Epoch 82 : 73.09%\n",
      "Epoch 83: training loss = 1.2880903698629194\n",
      "Training Accuracy after Epoch 83 : 73.28%\n",
      "Epoch 84: training loss = 1.2818500976785507\n",
      "Training Accuracy after Epoch 84 : 73.43%\n",
      "Epoch 85: training loss = 1.2756961832437221\n",
      "Training Accuracy after Epoch 85 : 73.62%\n",
      "Epoch 86: training loss = 1.269626645499331\n",
      "Training Accuracy after Epoch 86 : 73.76%\n",
      "Epoch 87: training loss = 1.2636726765660173\n",
      "Training Accuracy after Epoch 87 : 73.96%\n",
      "Epoch 88: training loss = 1.2578187212219911\n",
      "Training Accuracy after Epoch 88 : 74.18%\n",
      "Epoch 89: training loss = 1.2520242416618892\n",
      "Training Accuracy after Epoch 89 : 74.4%\n",
      "Epoch 90: training loss = 1.2462761317990743\n",
      "Training Accuracy after Epoch 90 : 74.53%\n",
      "Epoch 91: training loss = 1.240601631522413\n",
      "Training Accuracy after Epoch 91 : 74.74%\n",
      "Epoch 92: training loss = 1.2350321139493616\n",
      "Training Accuracy after Epoch 92 : 74.9%\n",
      "Epoch 93: training loss = 1.2295168614003888\n",
      "Training Accuracy after Epoch 93 : 75.05%\n",
      "Epoch 94: training loss = 1.224039173871063\n",
      "Training Accuracy after Epoch 94 : 75.2%\n",
      "Epoch 95: training loss = 1.2186148809750383\n",
      "Training Accuracy after Epoch 95 : 75.43%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: training loss = 1.2132471369402602\n",
      "Training Accuracy after Epoch 96 : 75.6%\n",
      "Epoch 97: training loss = 1.2079375693989145\n",
      "Training Accuracy after Epoch 97 : 75.75%\n",
      "Epoch 98: training loss = 1.2026723427797898\n",
      "Training Accuracy after Epoch 98 : 75.9%\n",
      "Epoch 99: training loss = 1.197438665002981\n",
      "Training Accuracy after Epoch 99 : 76.07%\n",
      "Epoch 100: training loss = 1.1922244670472606\n",
      "Training Accuracy after Epoch 100 : 76.24%\n"
     ]
    }
   ],
   "source": [
    "W2,b2,W1,b1=training(X_trainval,y_trainval,128,0.01,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Validation data using configuration 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Validation data: 75.3%\n"
     ]
    }
   ],
   "source": [
    "labely_test,labelpred=testing(W2,b2,W1,b1,X_testval,y_testval)\n",
    "accuracy_val=accuracy(labely_test, labelpred)\n",
    "print(\"Accuracy on Validation data: {}%\".format(accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
